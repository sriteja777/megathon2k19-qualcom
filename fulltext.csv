paper_text
"Anti-spoofing: Iris Databases

a
Joint Research Centre, European Commission, Ispra, Italy
b
Cyberisk Limited, London, UK

Synonyms
Liveness detection; Presentation attack detection; Spoofing countermeasures; Spoof detection;
Spoof resistance; Vitality tests

Definition
Anti-spoofing may be defined as the pattern recognition problem of automatically differentiating
between real and fake biometric samples produced with a synthetically manufactured artifact
(e.g., iris photograph or plastic eye). As with any other machine learning problem, the availability
of data is a critical factor in order to successfully address this challenging task. Furthermore, such
data should be public, so that the performance of different protection methods may be compared
in a fully fair manner. This entry describes general concepts regarding spoofing dataset acquisition
and particularizes them to the field of iris recognition. It also gives a summary of the most important
features of the public iris spoofing databases currently available.

Introduction
One of the key challenges faced by the rapidly evolving biometric industry is the need for publicly
available standard datasets that permit the objective and reproducible evaluation of biometric
recognition systems (e.g., performance, security, interoperability, or privacy). This is particularly
relevant for the assessment of spoofing attacks and their corresponding anti-spoofing protection
methodologies.
In relation to spoofing, the biometric community has started only recently to devote some
important efforts to the acquisition of large and statistically meaningful anti-spoofing databases.
In most cases, these datasets have been generated in the framework of international evaluation
competitions such as the recent Iris Liveness Detection Competition first held in 2013, the series
of Fingerprint Liveness Detection Competitions, LivDet, held biannually since 2009, or the 2D
Face Anti-Spoofing contests that started in 2011. Such initiatives provide public and common
benchmarks for developers and researchers to objectively evaluate their proposed anti-spoofing
solutions and compare them in a fair manner to other existing or future approaches. This way,
the public availability of standardized datasets is fundamental for the evolution of state-of-the-art
solutions.

In spite of the increasing interest in the study of vulnerabilities to direct attacks, the availability
of such spoofing databases is still scarce. This may be explained from both a technical and a legal
point of view:
• From a technical perspective, the acquisition of spoofing-related data presents an added
challenge to the usual difficulties encountered in the acquisition of standard biometric databases
(i.e., time-consuming, expensive, human resources needed, cooperation from the donors, etc.):
the generation of a large amount of fake artifacts which are in many cases tedious and slow to
generate on large scale (e.g., printed iris lenses).
• The legal issues related to data protection are controversial and make the sharing and distribution
of biometric databases among different research groups or industries very tedious and difficult.
These legal restrictions have forced most laboratories working in the field of spoofing to acquire
their own proprietary (and usually small) datasets on which to evaluate their protection methods.
Although these are valuable efforts, they have a limited impact, since the results may not be
compared or reproduced by other researchers.
Both public and proprietary datasets acquired for iris anti-spoofing evaluation have been
constructed following one of these three approaches:
• Different real/fake users. The spoofing database is constructed using the real samples of a
previously existing dataset. Then, fake samples of different new users are added. Anti-spoofing
is a two-class classification problem; therefore, from a theoretic point of view, such an approach
is valid for the evaluation of liveness detection techniques, as the database contains samples
of both classes. However, this type of database is not advisable and should be avoided, as it
presents two major problems: on the one hand, it has the fundamental limitation of not allowing
vulnerability studies of spoofing attacks where the intruder tries to access the system using a fake
biometric trait of a genuine user (as real and fake samples do not coincide) and, on the other
hand, real and fake samples do not only correspond to different persons but may also have been
acquired with a different sensor, at a different location, or following a different protocol, which
could potentially lead to biased results. Examples of works using such databases are commonly
found in iris spoofing-related literature [1, 12, 13].
• Same real/fake users, but different acquisition conditions. As in the previous case, the
spoofing database is constructed based on the real samples of a previous standard dataset.
However, in this case, those real samples are the ones used to produce the fake spoofs;
consequently, both real and fake users coincide. This could be, for instance, the case of an
iris spoofing database where the artifacts used to carry out the fraudulent access attempts are
printed photographs of an already publicly available iris image database. Again, the problem in
this case is that the results of an anti-spoofing evaluation may be biased due to changes in the
acquisition environment (e.g., sensor, illumination, distance to the sensor, pose, size, resolution,
etc.). In such conditions, the liveness detection algorithm may be detecting those contextual
variations and not the intrinsic differences between real and fake samples. Examples of works
using such databases in the iris domain include [2, 14].
• Same real/fake users and same acquisition conditions. This is the most advisable way to
proceed in an anti-spoofing evaluation. In this case, the database is generated from scratch for
the same real and fake users, under the same acquisition environment. All competitive antispoofing evaluation campaigns follow this approach.

This entry gives an overview of the current publicly available anti-spoofing databases that may
be used for the development and evaluation of new protection measures against direct attacks in
the field of iris recognition.
Before reviewing the most widely used fake iris databases which are publicly available, a brief
summary of the most common spoofing techniques is presented. An overview on spoofing is
provided to support the rationale behind the design of the datasets described later.
For a more comprehensive and detailed reading on iris spoofing and related countermeasures,
please see the encyclopedia entry: “Anti-spoofing: Iris.”

Iris Spoofing
While iris recognition is one of the most accurate biometric technologies, it is also a younger
research field compared to, for instance, fingerprint or face. As a consequence, iris spoofing has
also a somewhat shorter tradition than that of other long-studied modalities. Almost all iris spoofing
attacks reported in the literature follow one of three trends:
• Photo attacks. From a chronological point of view, these were the first attacks to be reported in
the literature and they still remain popular, probably due to their great simplicity and, in many
cases, high success rate [8, 9]. They are carried out presenting a photograph of the genuine iris.
In the vast majority of cases, this image is printed on paper (i.e., print attacks), although it could
also be displayed on the screen of a digital device such as a mobile phone or a tablet (i.e., digital
photo attacks). A slightly more evolved version of the basic print attacks, which has also been
considered in specialized works, consists of cutting out the pupil from the printout and placing
it in front of the attacker’s real eye. This way, countermeasures based on features extracted from
this part of the eye lose much of their efficiency [10].
A more sophisticated variation of photo attacks is video attacks, which consist of the
presentation of an eye video (or even a face video) replayed on a multimedia device such as
a smartphone or a laptop. Although this type of attacks has been mentioned in different irisrelated works [6, 14], up to date no practical vulnerability evaluation against video attacks has
been publicly reported in the iris domain.
• Contact-lens attacks. These appeared as a further evolution of the classic photo attacks. In this
case, the pattern of a genuine iris is printed on a contact lens that the attacker wears during the
fraudulent access attempt [11]. Such attacks are very difficult to be recognized even by human
operators and represent a real challenge for automatic protection methods as all the contextual
and ancillary information of the iris corresponds to that of a living eye. In most cases, the impact
analysis of this vulnerability has been carried out in the context of wider studies working on the
development of appropriate anti-spoofing approaches for these artifacts [6, 12–14].
• Artificial-eye attacks. These are far less common than the previous two types and have just
started to be systematically studied [3, 14]. Although some works may be found where very
sophisticated spoofing artifacts are presented, such as the use of multilayered 3D artificial irises
[7], in most cases, these attacks are carried out with artificial eyes made of plastic or glass.
Anti-spoofing methods based on the analysis of depth properties of the eye are more prone to be
deceived by such 3D reproductions.

Iris Spoofing Databases
Compared to other modalities such as fingerprint or face, iris is still a step behind in terms of the
organization of competitive liveness detection evaluations and also regarding the public availability
of spoofing data. In this context of limited resources, several studies carried out in the field of iris
security against direct attacks have been performed using samples from previously acquired real
datasets, so that in some cases real and fake users do not coincide [1, 12, 13].
In fact, until 2013, only one public iris spoofing database, the ATVS-FIr DB, was available [9].
In addition, its practical use was limited as it only related to one type of attack (i.e., print
attacks without cutting out the pupil) acquired with one sensor. The organization of the first
Liveness Detection-Iris Competition of 2013 (LivDet-Iris 2013) that considered the submission
of algorithms and systems [4] has notably improved iris data availability. The database used in the
contest comprises three different subsets of print and contact-lens attacks and it is significantly
larger than its predecessor.

ATVS-FIr DB
The ATVS-FIr DB [9] is publicly available at the ATVS-Biometric Recognition Group website
(http://atvs.ii.uam.es/).
The database comprises real and fake iris images (printed on paper) of 50 users randomly
selected from the BioSec baseline corpus. It follows the same structure as the original BioSec
dataset; therefore, it comprises 50 users 2 eyes 4 images 2 sessions D 800 fake iris images
and its corresponding original samples. The acquisition of both real and fake samples was carried
out using the LG IrisAccess EOU3000 sensor with infrared illumination which captures bmp
grayscale images of 640 480 pixels.
The fake samples were acquired following a three-step process which is further detailed in [9]:
(i) first, original images were processed to improve the final quality of the fake irises, (ii) then
they were printed using a high-quality commercial printer, and lastly (iii) the printed images were
handheld when presented to the iris sensor.
Although the database does not have an official protocol, in the experiments described in [5], the
database was divided into a training set, comprising 400 real images and their corresponding fake
samples of the first 50 eyes, and a test set with the remaining 400 real and fake samples captured
from the other 50 eyes available in the dataset.

LivDet-Iris DB
The first Liveness Detection-Iris Competition (LivDet-Iris) was held in 2013 [4]. The LivDet-Iris
2013 DB used in the evaluation will be distributed from the competition website (http://people.
clarkson.edu/projects/biosal/iris/index.php) once the official results are publicly released.
The database comprises over 4,000 samples acquired from around 500 different irises and is
divided into three datasets captured at three different universities: University of Notre Dame,
University of Warsaw, and Clarkson University. Each dataset was captured with a different sensor:

• IrisAccess LG4000 for the University of Notre Dame dataset.
• EyeGuard AD100 for the University of Warsaw dataset.
• Genie TS from Teledyne DALSA for the Clarkson University dataset. This is the only sensor
that captures video; however, in the dataset, only individual frames are included.
Two different types of spoof attacks are considered in the database: (i) print attacks, corresponding to the University of Warsaw dataset, and (ii) contact-lens attacks, contained in the Clarkson
University and the University of Notre Dame datasets. In addition, the Clarkson University dataset,
captured with a video camera, contains video frames that range from perfectly focused images to
samples with a ˙10 % focus deviation resulting in a varying level of blur (see Fig. 1 for graphical
examples).
The training and test sets that will be released are the same as the ones used in the LivDetIris 2013 competition so that future results achieved using it may be directly compared to those
obtained by the participants in the contest.

Summary
The establishment of public evaluation benchmarks is fundamental for the development of efficient
anti-spoofing countermeasures. The access to large databases permits a fair comparison between
security protection methods and the evolution of state-of-the-art solutions. However, technical and
legal difficulties associated with the collection of such data have slowed the development, and only
two iris spoofing databases are publicly available today.
Although the organization of the 2013 LivDet-Iris competition was a significant step forward
regarding the public availability of iris spoofing data, further efforts are still necessary before
iris technology reaches the same level as other biometric modalities. In particular, the LivDet-Iris
DB can still be complemented with data from additional subjects and/or collected under different
conditions in order to increase its variability. In addition, a new subset containing samples of
artificial-eye attacks (e.g., carried out with fake eyeballs) is yet to be generated.

Related Entries
Anti-spoofing:Face Databases
Anti-spoofing:Fingerprint Databases
Anti-spoofing:Iris

References
1. R. Bodade, S. Talbar, Dynamic iris localisation: a novel approach suitable for fake iris
detection. Int. J. Comput. Inf. Syst. Ind. Manage. Appl. 2, 163–173 (2010)
2. R. Bodade, S. Talbar, Fake iris detection: a holistic approach. Int. J. Comput. Appl. 19, 1–7
(2011)
3. R. Chen, X. Lin, T. Ding, Liveness detection for iris recognition using multispectral images.
Pattern Recognit. Lett. 33, 1513–1519 (2012)

4. Clarkson University, LivDet-Iris 2013: liveness detection-iris competition (2013), Available
online: http://people.clarkson.edu/projects/biosal/iris/
5. J. Galbally, J. Ortiz-Lopez, J. Fierrez, J. Ortega-Garcia, Iris liveness detection based on
quality related features, in Proceedings of the International Conference on Biometrics (ICB),
New Delhi, 2012, pp. 271–276
6. X. He, Y. Lu, P. Shi, A new fake iris detection method, in Proceedings of the IAPR/IEEE
International Conference on Biometrics (ICB), Alghero. LNCS, vol. 5558 (Springer, 2009),
pp. 1132–1139
7. A. Lefohn, B. Budge, P. Shirley, R. Caruso, E. Reinhard, An ocularist’s approach to human iris
synthesis. IEEE Trans. Comput. Graphics Appl. 23, 70–75 (2003)
8. T. Matsumoto, Artificial irises: importance of vulnerability analysis, in Proceedings of the
Asian Biometrics Workshop (AWB), vol. 45, 2004
9. V. Ruiz-Albacete, P. Tome-Gonzalez, F. Alonso-Fernandez, J. Galbally, J. Fierrez,
J. Ortega-Garcia, Direct attacks using fake images in iris verification, in Proceedings of the
COST 2101 Workshop on Biometrics and Identity Management (BioID), Roskilde. LNCS,
vol. 5372 (Springer, 2008), pp. 181–190
10. L. Thalheim, J. Krissler, Body check: biometric access protection devices and their programs
put to the test, c’t Magazine, Nov 2002, pp. 114–121
11. U.C. von Seelen, Countermeasures against iris spoofing with contact lenses, in Proceedings of
the Biometrics Consortium Conference, Arlington, Virginia, 2005
12. Z. Wei, X. Qiu, Z. Sun, T. Tan, Counterfeit iris detection based on texture analysis, in
Proceedings of the IEEE International Conference on Pattern Recognition (ICPR), Tampa,
2008
13. H. Zhang, Z. Sun, T. Tan, Contact lens detection based on weighted LBP, in Proceedings of the
IEEE International Conference on Pattern Recognition (ICPR), Istanbul, 2010, pp. 4279–4282
14. H. Zhang, Z. Sun, T. Tan, J. Wang, Learning hierarchical visual codebook for iris liveness
detection, in International Joint Conference on Biometrics, Washington DC, 2011"
"Convolutional Neural Networks for Iris Presentation Attack Detection: Toward
Cross-Dataset and Cross-Sensor Generalization
Steven Hoffman, Renu Sharma, Arun Ross∗
Department of Computer Science and Engineering
Michigan State University, USA
{hoffm470, sharma90, rossarun}@msu.edu

Abstract

detection (PAD) module is, therefore, essential to maintain system integrity. Many possible attacks have been
noted in the literature (Figure 1) based on printed iris images [15, 9], plastic, glass, or doll eyes [15, 28], cosmetic
contact lenses [15, 9, 23], replaying a video of an individual’s eye [3, 22], cadaver eyes [3, 19], robotic eyes [14],
holographic eye images [3, 19], or coercing an individual
to present their iris against their will [3, 19]. An ideal PAD
technique should be able to detect all of these PAs along
with any new or unknown PAs that may be developed in the
future. Although a number of iris PAs have been described,
current literature and publicly available datasets focus primarily on three main PAs — printed images, plastic eyes,
and cosmetic contacts [28, 8, 16, 32, 33, 4, 6] — possibly
due to their ease of construction and affordability.
PAD methods can be divided into two overarching categories: hardware-based and software-based. Hardware
methods employ additional sensors or equipment, besides
the iris device itself, in order to detect a PA. Software methods, on the other hand, use the image and other related information acquired by the iris device in order to detect a
PA. Several hardware solutions have been proposed in the
literature. Pacut and Czajka [3, 19] used the pupil’s natural reaction to light stimulus to detect printed images, while
Connell et al. [2] used structured light to distinguish between the 3D shape of live irides and cosmetic contacts.
Both Lee et al. [15, 16] and Park and Kang [20] examined
the differences between the reflected light of live and PA
samples under multiple bands of near-infrared (NIR) light.
Komogortsev et al. [14, 25] used eye tracking hardware to
distinguish between the movements of a live eye and that of
a printed image.
A number of software-based methods have also been
proposed in the literature. Many researchers have investigated the use of local texture descriptors — such as LBP,
LPQ, and BSIF — in conjunction with a Support Vector
Machine or other classifiers [9, 31, 22, 11, 6, 5, 13]. Image
quality features [8] and image frequency analysis [19] have
also been used to develop iris PAD schemes. Menotti et

Iris recognition systems are vulnerable to presentation
attacks where an adversary employs artifacts such as 2D
prints of the eye, plastic eyes, and cosmetic contact lenses
to obfuscate their own identity or to spoof the identity of
another subject. In this work, we design a Convolutional
Neural Network (CNN) architecture for presentation attack
detection, that is observed to have good cross-dataset generalization capability. The salient features of the proposed
approach include: (a) the use of the pre-normalized iris
rather than the normalized iris, thereby avoiding spatial information loss; (b) the tessellation of the iris region into
overlapping patches to enable data augmentation as well
as to learn features that are location agnostic; (c) fusion
of information across patches to enhance detection accuracy; (d) incorporating a “segmentation mask” in order
to automatically learn the relative importance of the pupil
and iris regions; (e) generation of a “heat map” that displays patch-wise presentation attack information, thereby
accounting for artifacts that may impact only a small portion of the iris region. Experiments confirm the efficacy of
the proposed approach.

1. Introduction
Iris biometric systems exploit the textural nuances of the
iris in order to recognize individuals in an automated manner [26]. Despite their increasing popularity, iris recognition systems are vulnerable to presentation attacks [17,
3, 25]. A presentation attack (PA)1 occurs when an adversarial user presents a fake or altered biometric sample to the sensor in order to spoof another user’s identity, obfuscate their own identity, or create a virtual identity. Developing a robust and accurate presentation attack


(a) Print

(b) Plastic

(c) Cosmetic Contacts


2. Proposed Method
In recent years, CNNs have achieved state-of-the-art performance on many computer vision tasks, including biometrics [29, 21]. Nonetheless, the literature on using CNNs for
iris PAD is relatively sparse [18, 10, 23]. Further, no analysis has been conducted to determine the generalizability
of these CNNs across different types of PAs and sensors.
Here, we discuss the design of our iris PAD CNN in order
to improve upon the existing work while also showing the
cross-dataset capabilities of CNNs.

2.1. Data Preprocessing
Iris datasets used in biometrics research typically contain images that exhibit additional ocular details besides the
iris, as seen on the left of Figure 2. However, in some PAs,
such as cosmetic contacts, only the iris region will contain
the PA information; the rest of the ocular region is unlikely
to manifest any trace of the artifact. Therefore, we segment
and localize the iris region to reduce confounding ocular information. The USIT segmentation tool [24] was used to facilitate segmentation whenever needed in this work. Since
the iris size in an ocular image varies significantly within
and across datasets, we resize all cropped iris images to

300×300 pixels, thereby offering a consistently sized input to the learning algorithm. A size of 300×300 was chosen so that the vast majority of images considered in this
work would be upsampled during resizing, as downsampling causes potentially important information to be lost.
Finally, we tessellate the segmented and resized iris image
into 25 overlapping patches of size 96×96. The primary
reason for this tessellation is for data augmentation; many
of the iris PAD datasets do not contain sufficient number
of data samples to effectively train a deep network. Square
patches are used to capture local texture information without having to make assumptions on the type of PA being
presented. We tested three different patch sizes — 48×48,
96×96, and 128×128 — and found that 96×96 patches
perform the best; see Section 4.3 for more details. The
patches overlap and, therefore, if important textural information were to lie along the edge of one patch, they will
be fully contained in the adjacent patch. The whole preprocessing pipeline is displayed in Figure 2. Finally, after
obtaining a PA score for each of the 25 patches, we employ
various fusion techniques, detailed in Section 2.3, to obtain
a single PA score for the iris.
Novelty with respect to other CNN-based schemes:

Our preprocessing differs from that used for CNNs elsewhere [18, 10, 23]. Menotti et al. input the full ocular image into their CNN as they were only trying to identify paper prints [18]. This, however, would not be appropriate for
identifying certain PAs, such as cosmetic contacts, which
only affect the iris region. He et al. [10] and Raghavendra et
al. [23] input patches of the normalized2 iris image to their
CNNs. In order to retain as much information in the data as
possible, we choose to input patches of the unnormalized
iris.

2.2. CNN Design Rationale
Our CNN design utilizes a set of eight convolutional layers, four max pooling layers, and a fully connected layer
with a ReLU non-linearity function following each convolutional layer. The CNN takes as input a single iris patch,
as detailed in Section 2.1, and outputs a PA score. A PA
score is a number in the range [0, 1] with 1 indicating a high
confidence that the input contains a PA. A Euclidean loss
function was chosen to train this network instead of the traditional softmax loss since the former allows for the direct
calculation of confidence scores, rather than just generating
class labels. Furthermore, our preliminary tests indicated
that the Euclidean layer performed better than the softmax
layer. The CNN architecture can be seen in Figure 3. The
design of our CNN is inspired by the state-of-the-art VGG
net [27]. However, to account for the small size of the input
iris patch and the availability of limited training data, we
used a shallow version of the VGG net.
Our CNN model also judiciously accounts for the number of iris and pupil pixels present in the input patch. When
detecting iris presentation attacks, it is important to account
for this since we focus primarily on attacks that can spoof or
obfuscate the iris region. For instance, an adversary launching a printed iris attack may cut out the pupil region from the
printed iris and place the print in front of their eyes in order
to confound sensors that look for specular reflection in the
pupil region [25]. Similarly, attacks based on cosmetic contacts may not obscure the pupil. Thus, pupil pixels seem
less likely to supply useful information for discriminating
PAs from live samples3 . To accommodate this observation,
we added a second channel to our CNN’s input which we
refer to as a segmentation mask (segmask). We define a
segmask as a 2-dimensional matrix that is of the same size
as the input iris patch. For every pixel location that corresponds to the iris region in the patch, the segmask contains
a value of +1, and for every pixel corresponding to a the
pupil region, the segmask contains a value of −1. For all
2 Here, normalization refers to the unwrapping of the iris wherein it is
mapped from Cartesian coordinates to Pseudo-polar coordinates resulting
in a fixed-size rectangular entity.
3 The term “live” is used to indicate that the iris being presented is real
and unmodified. In some literature, this is referred to as “bonafide”.

other pixels in the patch (e.g. sclera, eyelids, etc.) the segmask contains a value of 0. By adding this segmask as a
second input channel, the CNN can learn the importance of
each region of the iris image automatically without introducing any additional human bias. An example segmask is
shown in Figure 3.
The key aspects of our CNN model are summarized
here: (1) The CNN takes iris patches as input rather than
the full iris or ocular image thereby facilitating data augmentation during the training phase. (2) The input iris
patches are taken from the unnormalized iris image rather
than the normalized iris to avoid the downsampling that occurs during iris normalization. (3) A single CNN is trained
on patches originating from all parts of the cropped iris image, and, as such, the CNN does not attempt to learn location artifacts but focuses on PA artifacts. (4) Domainspecific knowledge is incorporated by accounting for the
number of iris and pupil pixels in an input patch through the
inclusion of segmentation masks in the input and through
defining patch-level fusion functions, as will be seen in Section 2.3.

2.3. Fusion Techniques
As described in Section 2.1, each ocular image is tessellated into 25 patches, and each one of these 25 patches produces its own score after passing through the CNN. However, a fusion method is needed in order to consolidate
the 25 scores and render a decision. One possible
fusion
PK
1
method is to take the average score, sav = K
i=1 si ,
where sav is the average score, si is the score of the ith
iris patch, and K is the total number of patches per image
(K = 25 in this case). Note that sav ∈ [0, 1], where 0
indicates a live sample and 1 indicates a PA.
As mentioned in Section 2.2, it is likely that the percentage of iris and pupil pixels in a patch will affect the PA
score. We designed two score fusion techniques based on
this intuition, the iris-only ratio (io) score and the iris-pupil
ratio (ip) score:

(2)
where, sio and sip are the iris-only ratio and iris-pupil ratio scores, respectively, si is the score of the ith iris patch,
ai and bi are the proportion of iris pixels and the ratio of
pupil pixels in the ith patch, respectively, K is the total
number of patches per image, [·][−1,+1] is a function that
converts scores to a [−1, +1] range, and [·][0,1] is a function

that converts scores to a [0, 1] range. During this computation, we first convert the scores to a [−1, +1] range so that
patches normally having a score of 0 will now have a score
of −1 and will be affected by the ratio of iris and pupil pixels. We divide by 1 + bi rather than bi itself to account for
those cases where bi = 0. This produces the iris-only (io)
score, which is a weighted average giving higher priority
to a patch’s score if it contains a larger proportion of iris
pixels, and the iris-pupil (ip) score, which is a weighted average giving higher priority to a patch’s score if it contains
more iris pixels and less priority if it contains more pupil
pixels. These fusion techniques do not directly take into
account the spatial location of the patch; rather they only
consider the proportion of iris or pupil contained in a patch.

number of images that were originally available in each of these datasets and the number that remained after excluding
those images which were not segmented. When training the CNNs, the training partition of each dataset was first balanced to ensure an equal number of live and PA samples; this was accomplished by randomly removing an appropriate number of training samples from the larger class. The
rest of this section gives further details on each dataset.

3.1. LivDet-Iris 2015 Warsaw Dataset

2.4. Evaluation Metrics
We report the True Detection Rate (TDR) and False Detection Rate (FDR), as defined below:

3. Datasets
The proposed method was evaluated on three publicly
available datasets: the LivDet-Iris 2015 Warsaw dataset, the
CASIA-Iris-Fake dataset, and the BERC-Iris-Fake dataset.
Since segmentation information was not provided with all
these datasets, we used automatic segmentation software to
locate the irides and pupils in the images. Whenever the
software failed to locate the iris and pupil within an image,
it was removed from the dataset.4 Table 1 summarizes the
4 We chose to discard these images, rather than manually segment them,
in order to have a fully automated PAD system. Future work will handle

Figure 4: Example images from the LivDet-Iris 2015 Warsaw dataset.

The LivDet-Iris 2015 dataset was developed for the 2015
Iris Liveness Detection Competition [33]. The Warsaw subset used in this work contains images of both live irides and
printed PAs. Note that the authors of this dataset claim that
it should contain 852 live images and 815 printed images
for training [33]; however, due to an error during data acquisition, we only gained access to 603 live and 582 printed
images from this set. Since the missing images were not acquired at the time of writing this paper, we trained only on
a subset of the LivDet-Iris Warsaw 2015 dataset. 

3.2. CASIA-Iris-Fake Dataset

two separate wavelengths: 750nm and 850nm; we pool the
images from these two spectral bands into a single set. Second, this dataset it has two separate types of print attacks:
those created using an inkjet printer and those created using
a laserjet printer; we pool these two sets of images into one
set as well. This dataset also provides an additional challenge in that its images appear to be preprocessed. Visual
inspection shows that all the printed and contact PAs have a
black circle placed over the pupil to mask both the pupil and
any specular reflections that may have otherwise been seen
in the pupil region. Furthermore, a transparent black circle
with a synthesized specular reflection has been placed over
the pupil of about half of the plastic PA images. Finally, this
dataset does not have separate training and testing subsets,
so we partitioned the data ourselves using the same procedure as for the CASIA-Iris-Fake dataset. Example images
from this dataset can be seen in Figure 6.

The CASIA-Iris-Fake dataset [28] is another dataset that
has been used for iris PAD evaluation. In contrast to the
LivDet Warsaw set, which only contains the printed iris PA,
the CASIA-Iris-Fake contains three of the most commonly
discussed PAs in the literature: printed, plastic, and cosmetic contacts. Note that CASIA does not provide separate
train and test partitions; rather, all the images are grouped
together in a single partition. Therefore, we partitioned the
dataset ourselves into training and testing subsets. Since
subject information was not provided, we used visual inspection of images and corresponding file names to do our
best in ensuring that subjects in the training and test sets
were mutually exclusive. Also, the images in this dataset
were already pre-cropped to the iris region. Example images from this dataset can be seen in Figure 5.

3.3. BERC-Iris-Fake Dataset
The BERC-Iris-Fake dataset [15, 16] also contains three
of the most commonly used PAs in the literature — printed,
plastic, and cosmetic contacts — making it useful for iris
PAD evaluation. This dataset is unique for two reasons.
First, rather than providing images captured at only a single NIR wavelength, the BERC dataset provides images at
these discarded images in an automated fashion.

4. Results
We trained a CNN on each dataset separately5 using
the MatConvNet toolkit [30], experimenting with different batch sizes and learning rates to optimize performance.
When a dataset contained multiple PAs (i.e., CASIA and
5 This resulted in three CNNs, which we refer to as LivDet-CNN,
CASIA-CNN and BERC-CNN in this paper. Note that each CNN was
trained using only the training partition of the corresponding dataset.

BERC), we trained a single CNN on all PA types to increase
the model’s generalizability across PA types. We also balanced the number of live and PA samples during training.
During testing, however, we evaluated the performance of
each PA type separately to obtain more insight into the performance of our CNN on each type of PA.6
We report TDR values at a FDR value of 1%, using the
iris-pupil ratio fusion technique as described in Section 2.3.
We chose this fusion technique as it produced better results
than both the average score and the iris-only ratio score on
the LivDet and BERC datasets. On the CASIA dataset, the
average score performed the best and, therefore, this score
is reported on intra-dataset tests involving CASIA.
In the case of the BERC-CNN, we found that the automatic segmentation results on the training set were poor,
not being able to localize the iris and pupil as proficiently
compared to the other two datasets. This caused the CNN
trained with segmentation masks to perform worse than the
same CNN trained without segmentation masks. The results on BERC are, therefore, from the CNN without using
the segmask.
When a CNN is tested on the same dataset that it was
trained on, we call this an intra-dataset evaluation; when
the same CNN is evaluated on a dataset that is different from
what it was trained on, we call this a cross-dataset evaluation. In all cases, subjects in the training and test sets are
mutually exclusive.

4.1. Intra-Dataset Results
Our intra-dataset results7 are shown in Table 2. The authors of the Federico model [33] had access to the complete LivDet-Iris 2015 training set, meaning they were able
to train on a larger dataset than we could. Nonetheless,
our model is able to achieve comparable results with the
current state-of-the-art, showing the efficacy of our model.
Furthermore, the authors of the BERC-H model [16] used
a hardware-based method to perfectly classify the BERCIris-Fake dataset, relying on a sensor to capture images at
both 750nm and 850nm in order for their system to classify
it as live or spoof. We were able to replicate their accuracy without taking into account the multiple spectra, using
a software-based solution. Our model, however, was unable
to reproduce the state-of-the-art for the CASIA-Iris-Fake
dataset at the FDR of 1%, particularly failing at identifying cosmetic contacts. Upon analyzing the CASIA dataset,
we found that it exhibits a higher degree of variability in
the PA samples than the LivDet and BERC datasets. In particular, it consisted of a large number of different cosmetic
6 Testing was conducted only on the test partition of the corresponding
dataset.
7 Note that we report results from training and testing only on those
images for which automatic segmentation was successful. We include an
asterisk next to the dataset name to indicate this.

contact styles. We speculate that our CNN model may have
been too shallow to efficiently capture this variability, causing the decreased performance on CASIA.
Table 2: The results of the proposed approach on each
dataset under intra-dataset conditions. We evaluated against
each PA type separately to get a better understanding of how
our CNNs work on each PA. The TDR@FDR= 1% is reported for print, plastic, and cosmetic contact (CC) PAs.

4.2. Cross-Dataset Results
In the literature, very little has been reported on the generalizability of iris PAD algorithms across PAs, sensors, or
datasets. A few authors have attempted cross-sensor testing
of their PAD algorithms [5, 23, 31]. These authors evaluated their models on datasets that used multiple sensor
brands. They trained their model on data from one sensor
and evaluated on data from the other sensors. Under these
scenarios, the TDR was often much lower than those seen
in intra-dataset conditions, and the FDR was much higher.
This highlights the difficulties in cross-sensor testing scenarios.
However, an even more difficult testing condition than
this is to perform cross-dataset testing. During cross-sensor
testing, one must primarily account for variations in cameras, but during cross-dataset testing, one must account for
variations in the sensors, data acquisition environment, subject population, and PA generation procedures. This is referred to as dataset biases [7, 1]. This makes cross-dataset
testing a very difficult problem, and to the best of our
knowledge, cross-dataset evaluation has not been conducted
so far for iris PAD methods. Nonetheless, a real-world scenario demands that iris PAD algorithms be able to operate
under the kind of variations seen in cross-dataset testing. To
perform cross-dataset evaluation, we took our best performing CNN from the intra-dataset testing scenario, viz., the
BERC-CNN, and tested it against the other two datasets.
The results can be seen in Table 3. This CNN achieved
high TDRs on both the LivDet-Iris Warsaw 2015 dataset
and the printed PAs of the CASIA-Iris-Fake dataset, returning TDRs of 97.69% and 100%, respectively, at an FDR
of 1%. The model did not perform as well on the plastic
or contact PAs of the CASIA dataset, achieving TDRs of
65.38% and 16.41% respectively. This suggests that crossPA testing represents a very challenging scenario.

4.3. Patch Size Analysis
In order to study the impact of patch size, we repeated
the above experiments, which use a patch size of 96×96,
with patch sizes of 48×48 and 128×128. We trained these
CNNs with batch normalization to improve convergence.
The results of the tests on the different patch sizes are
shown in Table 4. From this table, we can see that the
patch size of 96×96 provides the best results. The performance increases when expanding the patch size from
48×48 to 96×96, possibly because the CNN has more information in a patch to generate a PA score. However,
when increasing the patch size from 96×96 to 128×128,
the number of weights needed to train the CNN increases
and the amount of data we have available to train the network is likely too small, thereby decreasing the accuracy on
128×128 patches.

4.4. Feature Map Analysis
Table 3: The results of the proposed approach under
cross-dataset conditions when having trained on BERCIris-Fake* and tested on the remaining two datasets. The
TDR@FDR= 1% is reported for print, plastic and cosmetic
contact (CC) PAs.
Test Set

To gain a better understanding of what our CNN models learned, we analyzed the intermediate representations
of image patches as they passed through the CNN. We
looked at our best performing model, the CNN trained on
the BERC dataset, and viewed the outputs (feature maps)
of the 1st, 3rd, 5th, and 7th convolutional layers. Exemplar feature maps for a PA sample can be found in Figure 7,
while those for a live sample can be seen in Figure 8. An
analysis of these images reveals that the first few layers of
the CNN work as edge detectors, emphasizing edges formed
by both the iris texture and by the low resolution of the PA.
In particular, the outputs of the 1st and 3rd convolutional layers for the PA sample reveal a fine-grained grid pattern
due to the pixelation in the cosmetic contact lens. These
pixelations get smoothened out later in the CNN such that
by the 7th convolutional layer, the output feature maps exhibit only small variations in intensity. For the live samples,
however, a strong grid pattern is not seen in the first few feature maps. This leads the CNN to exhibit large variations in
image intensity in the feature maps corresponding to the 7th
layer, allowing the live samples to be discriminated from
PAs. This analysis suggests that our model should do well
at identifying PAs whose iris details are of lower resolution.
Conversely, when a PA sample has a higher resolution and
the inherent pixelation is not evident in the resulting image,
our model is not as likely to do well in PA detection.

4.5. Failure Case Analysis
To better understand our CNN trained models, we visually analyzed the misclassified images by the BERC CNN
on the cross-dataset experiments. We generated heatmaps
for each image, where the intensity of a pixel in this
heatmap corresponded to the average PA score of all patches
embodying that pixel. These heatmaps allowed us to better
visualize the differences in PA detection accuracy across an

(a) LivDetW15 Live: 0.65

(b) CASIA Live: 0.57

(c) LivDetW15 Print: 0.56

(d) CASIA Plastic: 0.40

the CASIA dataset at an FDR of 1%. For plastic PAs, this
is likely because the BERC dataset only contains 31 plastic images for training, so it was unable to learn a model
strong enough to identify this PA in another dataset, especially since the plastic eye images in the CASIA dataset
look much different than those used in BERC (compare Figure 5 and Figure 6). For the cosmetic contact PA [5], we
have already noted that the variations in the contacts in the
CASIA dataset are much higher than those in the BERC
dataset, accounting for a much lower TDR in both intradataset and cross-dataset testing. Nonetheless, our proposed
model was able to establish state-of-the-art results in the domain of cross-dataset iris PAD testing.

Acknowledgment

Figure 9: Images misclassified by the CNN trained on
BERC. The corresponding heatmaps, ground truth classification labels, and PA scores are also shown. Thresholds of
0.64 for testing on LivDet and 0.57 for testing on CASIA
were used to obtain the desired FDR of 0.2%.
image. Example heatmaps can be seen in Figure 9. A few
observations can be made. First, we noticed that a large
number of errors in both live and PA images occurred when
there was a significant amount of glare. Since glare obscures the underlying object’s texture, this artifact makes
PA classification much more difficult. We also noticed that
in some of the misclassified live images, such as the one
in Figure 9a, the texture of the iris was rather smooth, not
containing a lot of discriminative texture. The same can be
seen in some misclassified spoof images, like in Figure 9c,
where the PA sample has high contrast, making it difficult to
discern any texture. This is consistent with our feature map
analysis, suggesting that when neither the irregular texture
of a live iris nor the fine-grained grid pattern of PAs can be
identified by the CNN’s edge-detecting filters, the CNN is
unable to reliably classify the image.

5. Discussion
Upon analyzing the cross-dataset results, we see that the
proposed CNN was able to perform exceptionally well on
the LivDet-Iris Warsaw 2015 dataset and the printed PA
subset of the CASIA-Iris-Fake dataset. The CNN was able
to harness the power of deep learning to generate feature
extractors that generalize well across datasets. In addition,
because the BERC-Iris-Fake dataset contains images at two
different light spectra, we believe that the CNN was able
to extract more variations than that which naturally occurs
when training on only a single light spectrum. However, the
CNN did not perform well on plastic or contact PAs from

This research is based upon work supported in part by
the Office of the Director of National Intelligence (ODNI),
Intelligence Advanced Research Projects Activity (IARPA).
The views and conclusions contained herein are those of the
authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of
ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for
governmental purposes notwithstanding any copyright annotation therein.

References
[1] S. Banerjee and A. Ross. From Image to Sensor: Comparative Evaluation of Multiple PRNU Estimation Schemes for
Identifying Sensors from NIR Iris Images. In Fifth International Workshop on Biometrics and Forensics, 2017. 6
[2] J. Connell, N. Ratha, J. Gentile, and R. Bolle. Fake Iris Detection Using Structured Light. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),
pages 8692–8696, 2013. 1
[3] A. Czajka. Iris Liveness Detection by Modeling Dynamic
Pupil Features. In M. J. Burge and K. W. Bowyer, editors,
Handbook of Iris Recognition, volume 1542, chapter 19,
pages 439–467. Springer-Verlag, London, 2013. 1
[4] J. Doyle and K. Bowyer. Technical Report: Notre Dame Image Database for Contact Lens Detection in Iris Recognition2013, 2014. 1, 2
[5] J. S. Doyle and K. W. Bowyer. Robust Detection of Textured Contact Lenses in Iris Recognition Using BSIF. IEEE
Access, 3:1672–1683, 2015. 1, 2, 6, 8
[6] J. S. Doyle, K. W. Bowyer, and P. J. Flynn. Variation in Accuracy of Textured Contact Lens Detection Based on Sensor
and Lens Pattern. IEEE 6th International Conference on Biometrics: Theory, Applications and Systems (BTAS), 2013. 1,
2
[7] S. El-Naggar and A. Ross. Which Dataset is this Iris Image From? In IEEE International Workshop on Information
Forensics and Security (WIFS), pages 1–6, Nov 2015. 6

[8] J. Galbally, S. Marcel, and J. Fierrez. Image Quality Assessment for Fake Biometric Detection: Application to Iris,
Fingerprint, and Face Recognition. IEEE Transactions on
Image Processing, 23(2):710–724, 2014. 1
[9] D. Gragnaniello, G. Poggi, C. Sansone, and L. Verdoliva.
An Investigation of Local Descriptors for Biometric Spoofing Detection. IEEE Transactions on Information Forensics
and Security, 10(4):849–863, 2015. 1
[10] L. He, H. Li, F. Liu, N. Liu, Z. Sun, and Z. He. Multipatch Convolution Neural Network for Iris Liveness Detection. IEEE 8th International Conference on Biometrics: Theory, Applications, and Systems (BTAS), 2016. 2, 3
[11] Z. He, Z. Sun, T. Tan, and Z. Wei. Efficient Iris Spoof Detection via Boosted Local Binary Patterns. International Conference on Biometrics, 1(c):1087–1097, 2009. 1
[12] Hoover Vision Center.
Halloween Hazard:
The
Dangers
of
Cosmetic
Contact
Lenses,
http://hoovervisioncenter.com/2015/10/21/halloweenhazard-the-dangers-of-cosmetic-contact-lenses/,
visited:
2018-01-03. 2
[13] N. Kohli, D. Yadav, M. Vatsa, R. Singh, and A. Noore. Detecting Medley of Iris Spoofing Attacks using DESIST. In
IEEE 8th International Conference on Biometrics Theory,
Applications and Systems (BTAS), pages 1–6, 2016. 1
[14] O. V. Komogortsev, A. Karpov, and C. D. Holland. Attack
of Mechanical Replicas: Liveness Detection with Eye Movements. IEEE Transactions on Information Forensics and Security, 10(4):716–725, 2015. 1
[15] S. J. Lee, K. R. Park, and J. Kim. Robust Fake Iris Detection
Based on Variation of the Reflectance Ratio between the Iris
and the Sclera. BCC Biometrics Symposium, 2006. 1, 5
[16] S. J. Lee, K. R. Park, Y. J. Lee, K. Bae, and J. Kim.
Multifeature-based Fake Iris Detection Method. Optical Engineering, 46(12):127204, 2007. 1, 5, 6
[17] S. Marcel, M. S. Nixon, and S. Z. Li. Handbook of Biometric
Anti-Spoofing. 2014. 1
[18] D. Menotti, G. Chiachia, A. Pinto, W. R. Schwartz,
H. Pedrini, A. X. Falcao, and A. Rocha. Deep Representations for Iris, Face, and Fingerprint Spoofing Detection.
IEEE Transactions on Information Forensics and Security,
10(4):864–879, 2015. 2, 3
[19] A. Pacut and A. Czajka. Aliveness Detection for Iris Biometrics. In 40th Annual IEEE International Carnahan Conferences on Security Technology (ICCST), pages 122–129.
IEEE, 2006. 1
[20] J. H. Park and M. G. Kang. Iris Recognition Against Counterfeit Attack Using Gradient Based Fusion of Multi-spectral
Images. Engineering, pages 150–156, 2005. 1

[21] O. M. Parkhi, A. Vedaldi, and A. Zisserman. Deep Face
Recognition. In British Machine Vision Conference BMVC,
volume 1, page 6, 2015. 2
[22] R. Raghavendra and C. Busch. Robust Scheme for Iris Presentation Attack Detection using Multiscale Binarized Statistical Image Features. IEEE Transactions on Information
Forensics and Security, 10(4):703–715, 2015. 1
[23] R. Raghavendra, K. B. Raja, and C. Busch. ContlensNet:
Robust Iris Contact Lens Detection Using Deep Convolutional Neural Networks. In IEEE Winter Conference on Applications of Computer Vision (WACV), pages 1160–1167.
IEEE, 2017. 1, 2, 3, 6
[24] C. Rathgeb, A. Uhl, P. Wild, and H. Hofbauer. Design Decisions for an Iris Recognition SDK. In K. Bowyer and M. J.
Burge, editors, Handbook of Iris Recognition, Advances in
Computer Vision and Pattern Recognition. Springer, second
edition edition, 2016. 2
[25] I. Rigas and O. V. Komogortsev. Eye Movement-driven Defense Against Iris Print-attacks. Pattern Recognition Letters,
68(July):316–326, 2015. 1, 3
[26] A. Ross. Iris Recognition: The Path Forward. IEEE Computer, 43(2):30–35, Feb 2010. 1
[27] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. CoRR,
abs/1409.1556, 2014. 3
[28] Z. Sun, H. Zhang, T. Tan, and J. Wang. Iris Image Classification Based on Hierarchical Visual Codebook. IEEE
Transactions on Pattern Analysis and Machine Intelligence,
36(6):1120–1133, 2014. 1, 5
[29] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf. Deepface:
Closing the Gap to Human-level Performance in Face Verification. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 1701–1708, 2014. 2
[30] A. Vedaldi and K. Lenc. MatConvNet – Convolutional Neural Networks for MATLAB. In ACM Int. Conf. on Multimedia, 2015. 5
[31] D. Yadav, N. Kohli, J. S. Doyle, R. Singh, M. Vatsa, and
K. W. Bowyer. Unraveling the Effect of Textured Contact
Lenses on Iris Recognition. IEEE Transactions on Information Forensics and Security, 9(5):851–862, 2014. 1, 2, 6
[32] D. Yambay, J. S. Doyle, K. W. Bowyer, A. Czajka, and
S. Schuckers. LivDet-Iris 2013-Iris Liveness Detection
Competition 2013. pages 1–8. IEEE, 2014. 1
[33] D. Yambay, B. Walczak, S. Schuckers, and A. Czajka.
LivDet-Iris 2015 - Iris Liveness Detection Competition 2015.
In IEEE International Conference on Identity, Security, and
Behavior Analysis (ISBA). IEEE, 2017. 1, 4, 6"
"StaRe: Statistical Reasoning Tool for
5G Network Management
Kasper Apajalahti1 , Eero Hyvönen1 , Juha Niiranen2 , Vilho Räisänen3
Aalto University, Semantic Computing Research Group (SeCo), Finland
University of Helsinki, Department of Mathematics and Statistics, Finland
Nokia Networks Research, Finland

Abstract. In operations of increasingly complex telecommunication networks,
characterization of a system state and choosing optimal operation in it are challenges. One possible approach is to utilize statistical and uncertain information in
the network management. This paper gives an overview of our work in which a
Markov Logic Network model (MLN) is used for mobile network analysis with
an RDF-based faceted search interface to monitor and control the behavior of the
MLN reasoner. Our experiments, based on a prototype implementation, indicate
that the combination of MLN and semantic web technologies can be effectively
utilized in network status characterization, optimization and visualization.

Introduction

The growing complexity of telecommunication networks requires more automation
from the network management layer. Currently researched and standardized technology
in the telecommunication field is Self-Organizing Networks (SON) [1] which solves automatically some management tasks in a limited context using a fixed rule base. However, advanced uncertainty management beyond simple static rule bases is required to
combine high service quality with optimization of operational expenses [5]. For this
goal, we present a prototype tool StaRe that provides the user with a possibility to
understand the characterization of the autonomic network management system and its
uncertainties. The novel idea is to apply MLN [7] for mobile network analysis and management under uncertainty. We have examined how an ontology-based MLN model can
be effectively utilized by a human operator using a SPARQL endpoint and a faceted
browser GUI. It is crucial that the operator monitors and controls the system behavior
even when the autonomic system solves the majority of management tasks [8].

Prototype Architecture and Data Sequence

StaRe is a runtime environment tool that integrates dynamically an MLN model, an
ontology based on it, and a GUI for mobile network data analysis and management. A
Long-Term Evolution (LTE) simulator is used for simulating an urban mobile network
environment.
Fig. 1 depicts our architecture and its data sequence for managing a mobile network. The data sequence starts from the right where the simulator data is retrieved to the MLN model in every 15 minutes of simulation time. This data contains key performance indicators (KPI) for measurement cases, such as channel quality indicator (CQI)
and radio link failures (RLF). In return, the MLN model reasons configuration management parameters for the mobile network (i.e., the LTE simulator) that contain needed
changes in the transmission power (TXP) and angle (remote electrical tilt, RET) of a
cell antenna. These parameters are critical for the quality of service of the network and
operation optimization.
The simulator data is used as the evidence of the MLN model to infer posterior
probabilities for action proposals. Network cells in the simulator are then configured
based on the action proposal distributions. In order to make this process manageable
to the operator, the ontology processor retrieves the evidence, rules, action proposals,
and configurations, and constructs an ontology by parsing and mapping this data into
a network- and MLN-related semantic graph. This ontology is then uploaded into a
SPARQL server based on Fuseki1 . The server dynamically generates facets from the
ontology (with SPARQL update scripts) for the GUI and acts as a data storage both for
the GUI and MLN model. The GUI interacts with the SPARQL endpoint to retrieve
semantic data from the ontology and to update the semantic MLN rule base. Similarly,
the MLN model queries the SPARQL endpoint in order to retrieve updated rule base.

Faceted Browser Interface

The GUI is an HTML5 application which is built by using faceted browsing and interactive visualization methods in order to 1) determine needed network management
actions in a situation and to 2) manage the MLN rules. Fig. 2 shows how facet selections
can be used to search for recommended actions. Here cells with a high RLF value and
low CQI value are selected on the facets on the left, and the tool suggests as an action
proposal to increase the TXP of the cells on the right with a varying uncertainty.
Fig. 3 shows a view for managing the uncertain MLN rules by dividing each rule
into a rule weight and to semantically defined rule classes: context (current network
status), objective (desired change in the network status) and action (configurations for the network). The operator uses this view to investigate contents of the rules and to
manipulate the rule base in order to change the behaviour of the MLN reasoner.
The facets are generated as a combination of rule classes (context, objectives and
actions) and their objects (CQI, RLF, TXP, and RET). Here the operator has filtered
out rules containing low CQI in the context part and increase CQI in the objective part.
The result indicates that every single rule can be removed or its weight can be updated.
The possibility to remove a set of rules that satisfies current facet selections can be seen
above the result table.

In StaRe, the operator can also create new rules with a rule creation form which
enables a productive way to create MLN rules without writing the actual MLN syntax.

Related Work and Discussion

Various uncertain reasoning techniques have been applied to different network management tasks in the telecommunications field. For example, Bayesian networks (BN)
are proposed for automatic network fault management [4][2] and MLN to diagnose
anomalous cells [3]. Ontologies have also been used to model general concepts of the telecommunication field [6] as well as to model context in mobile network management [9][11]. The Linked Open Data (LOD)2 paradigm has also been addressed in
[10], which models cells and terminals, and combines them with other data sources,
for example with event data. However, there exists no research of using ontologies and
statistical reasoning together to analyze and configure the mobile network, as in this
paper.
Altogether, StaRe has proven to be a useful tool in complex network management
tasks as it contains a reasoner for processing uncertain information and a semantic
faceted browser interface for information exploration. As this paper shows, the StaRe
ontology can be used as a semantic data storage between the MLN reasoner and GUI.

References
1. 3GPP: Telecommunication management; Self-Organizing Networks (SON); Concepts and
requirements, [3GPP TS 32.500 Release 13, modified]. Tech. rep., 3rd Generation Partnership Project (3GPP) (Feb 2016), http://www.3gpp.org/DynaReport/32500.htm
2. Bennacer, L., Ciavaglia, L., Chibani, A., Amirat, Y., Mellouk, A.: Optimization of fault diagnosis based on the combination of Bayesian Networks and Case-Based Reasoning. In:
Network Operations and Management Symposium (NOMS). pp. 619–622. IEEE (2012)
3. Ciocarlie, G., Connolly, C., Cheng, C.C., Lindqvist, U., Nováczki, S., Sanneck, H., Naseer-ul
Islam, M.: Anomaly Detection and Diagnosis for Automatic Radio Network Verification. In:
Agüero, R., Zinner, T., Goleva, R., Timm-Giel, A., Tran-Gia, P. (eds.) Mobile Networks and
Management, Lecture Notes of the Institute for Computer Sciences, Social Informatics and
Telecommunications Engineering, vol. 141, pp. 163–176. Springer International Publishing
(2015), http://dx.doi.org/10.1007/978-3-319-16292-8 12
4. Hounkonnou, C., Fabre, E.: Empowering Self-diagnosis with Self-modeling. In: Proceedings
of the 8th International Conference on Network and Service Management. pp. 364–370.
International Federation for Information Processing (2012)
5. Hämäläinen, S., Sanneck, H., Sartori, C.: LTE Self-Organising Networks (SON): Network
Management Automation for Operational Efficiency. Wiley Online Library, 1st edn. (2012)
6. Qiao, X., Li, X., Chen, J.: Telecommunications service domain ontology: semantic interoperation foundation of intelligent integrated services. Telecommunications Networks-Current
Status and Future Trends pp. 183–210 (2012)
7. Richardson, M., Domingos, P.: Markov Logic Networks. Machine Learning 62(1-2), 107–
136 (Feb 2006)
8. Russell, D.M., Maglio, P.P., Dordick, R., Neti, C.: Dealing With Ghosts: Managing the User
Experience of Autonomic Computing. IBM Systems Journal 42(1), 177–188 (2003)
9. Stamatelatos, M., Yahia, I.G.B., Peloso, P., Fuentes, B., Tsagkaris, K., Kaloxylos, A.: Information Model for Managing Autonomic Functions in Future Networks. In: Mobile Networks
and Management, pp. 259–272. Springer (2013)
10. Uzun, A., Küpper, A.: OpenMobileNetwork: extending the web of data by a dataset for mobile networks and devices. In: Proceedings of the 8th International Conference on Semantic
Systems. pp. 17–24. ACM (2012)
11. Yahia, B., Grida, I., Bertin, E., Crespi, N.: Ontology-Based Management Systems for the
Next Generation Services: State-of-the-Art. In: Networking and Services, 2007. ICNS. Third
International Conference on. pp. 40–40. IEEE (2007)
2

http://linkeddata.org/"
"Meenal G. Kachhavay et al, International Journal of Computer Science and Mobile Computing, Vol.3 Issue.3, March- 2014, pg. 1080-1087

International Journal of Computer Science and Mobile Computing
A Monthly Journal of Computer Science and Information Technology

5G Technology-Evolution and Revolution
Meenal G. Kachhavay[1]

Ajay P.Thakare[2]

M.E. [I st year] C.S.E. Sipna College of Engineering, Amravati

Head of Department, Electronics and Telecommunication, Sipna College of Engineering, Amravati

--------------------------------------------------------------------------------------------------------------------Abstract: In this paper, an attempt has been made to review various existing generations of
mobile wireless technology in terms of their portals, performance, advantages and
disadvantages. The paper throws light on the evolution and development of various
generations of mobile wireless technology along with their significance and advantages of one
over the other. In the past few decades, mobile wireless technologies have experience 4 or 5
generations of technology revolution and evolution, namely from 1G to 4G.Current research
in mobile wireless technology concentrates on advance implementation of 4G technology and
5G technology. Currently 5G term is not officially used. In 5G research is being made on
development of World Wide Wireless Web (WWWW), Dynamic Adhoc Wireless Networks
(DAWN) and Real Wireless World. In this paper we propose novel network architecture for
next generation 5G mobile networks.. In the proposed architecture the mobile terminal has the
possibility to change the Radio Access Technology - RAT based on certain user criteria.
KEY CONCEPT: Evolution from 1G-5G, 5G Network Architecture, Need of 5G

Meenal G. Kachhavay et al, International Journal of Computer Science and Mobile Computing, Vol.3 Issue.3, March- 2014, pg. 1080-1087

I.

INTRODUCTION

Mobile wireless industry has started its technology creation, revolution and evolution since early 1970s. In the past
few decades, mobile wireless technologies have experience 4 or 5 generations of technology revolution and
evolution. [1] The telecommunication service in World had a great leap within last few years. 6 billion people own
mobile phones so we are going to analyze the various generations of cellular systems as studied in the evolution of
mobile communications from 1st generation to 5th generation. We can analyze that this could be due to increase in
the telecom customers day by day. In the present time, there are four generations in the mobile industry. These are
respectively 1G- the first generation, 2G- the second generation, 3G- the third generation, and then the 4G- the forth
generation,5G-the fifth second generation.[1] Now days different wireless and mobile technologies are present such
as third generation mobile networks (UMTS- Universal Mobile Telecommunication System, cdma2000), LTE
(Long Term Evolution), Wi-Fi (IEEE 802.11 wireless networks), WiMAX (IEEE 802.16 wireless and mobile
networks), as well as sensor networks, or personal area networks (e.g. Bluetooth, ZigBee). Mobile terminals include
variety of interfaces like GSM which are based on circuit switching. All wireless and mobile networks implements
all-IP principle, that means all data and signaling will be transferred via IP (Internet Protocol) on network layer.
Fifth generation technology provide facilities like camera, MP3 recording, video player, large phone memory, audio
player etc. that user never imagine and for children rocking fun with Bluetooth technology and Piconets. The fifth
generation wireless mobile multimedia internet networks can be completely wireless communication without
limitation, which makes perfect wireless real world – World Wide Wireless Web (WWWW). Fifth generation is
based on 4G technologies. The 5th wireless mobile internet networks are real wireless world which shall be
supported by LAS-CDMA (Large Area Synchronized Code-Division Multiple Access), OFDM (Orthogonal
frequency-division multiplexing), MCCDMA (Multi-Carrier Code Division Multiple Access), UWB (Ultrawideband), Network-LMDS (Local Multipoint Distribution Service), and IPv6. Fifth generation technologies offers
tremendous data capabilities and unrestricted call volumes and infinite data broadcast together within latest mobile
operating system. Fifth generation should make an important difference and add more services and benefits to the
world over 4G. Fifth generation should be more intelligent technology that interconnects the entire world without
limits. This generation is expected to be released around 2020. The world of universal, uninterrupted access to
information, entertainment and communication will open new dimension to our lives and change our life style
significantly. [2]

II.
EVOLUTION OF MOBILE TECHNOLOGIES
Mobile communication has become more popular in last few years due to fast revolution in mobile technology. This
revolution is due to very high increase in telecoms customers. This revolution is from 1G- the first generation, 2Gthe second generation, 3G- the third generation, and then the 4G- the forth generation,5G-the fifth second
generation. [2]
Meenal G. Kachhavay et al, International Journal of Computer Science and Mobile Computing, Vol.3 Issue.3, March- 2014, pg. 1080-1087

Fig : Evolutional Changes in Mobile Technologies[7]
A. First Generation(1G):
1G emerged in 1980s.It contains analog system and popularly known as cell phones. It introduces mobile
technologies such as mobile telephone system (MTS),Advanced mobile telephone system(AMTS),Improved mobile
telephone system(IMTS)and push to talk(PTT).It uses analog radio signal which have frequency 150 MHz, Voice
call modulation is done using a technique called frequency division multiple access(FDMA).It has low capacity ,
unreliable handoff, poor voice links and no security at all since voice calls were played back in radio towers making
these calls susceptible to unwanted eavesdropping by third parties[2].
B. Second Generation (2G):
2G emerged in late 1980s. It uses digital signals for voice transmission and has speed of 64 kbps. It provides facility
of SMS (Short Message Service) and use the bandwidth of 30 to 200 KHz. Next to 2G, 2.5G system uses packet
switched and circuit switched domain and provide data rate up to 144 kbps. E.g. GPRS, CDMA and EDGE [2]
C. Third Generation (3G):
It uses Wide Brand Wireless Network with which clarity is increased. The data are sent through the technology
called Packet Switching. Voice calls are interpreted through Circuit Switching. Along with verbal communication it
includes data services, access to television/video, new services like Global Roaming. It operates at a range of
2100MHz and has a bandwidth of 15-20MHz used for High-speed internet service, video chatting.3G uses Wide
Band Voice Channel that is by this the world has been contracted to a little village because a person can contact with
other person located in any part of the world and can even send messages too[2].
D. Fourth Generation (4G):
4G offers a downloading speed of 100Mbps.4G provides same feature as 3G and additional services like MultiMedia Newspapers, to watch T.V programs with more clarity and send Data much faster than previous generations
[3]. LTE (Long Term Evolution) is considered as 4G technology. 4G is being developed to accommodate the QoS
and rate requirements set by forthcoming applications like wireless broadband access, Multimedia Messaging
Service (MMS), video chat, mobile TV, HDTV content, Digital Video Broadcasting (DVB), minimal services like
voice and data, and other services that utilize bandwidth. [2]
E. Fifth Generation (5G):
5G Technology stands for 5th Generation Mobile technology. 5G mobile technology has changed the means to use
cell phones within very high bandwidth. User never experienced ever before such a high value technology.
Nowadays mobile users have much awareness of the cell phone (mobile) technology. The 5G technologies include
all type of advanced features which makes 5G mobile technology most powerful and in huge demand in near future.
A user can also hook their 5G technology cell phone with their Laptop to get broadband internet access. 5G
technology including camera, MP3 recording, video player, large phone memory, dialing speed, audio player and
much more you never imagine. For children rocking fun Bluetooth technology and Piconets has become in market
[1].


Meenal G. Kachhavay et al, International Journal of Computer Science and Mobile Computing, Vol.3 Issue.3, March- 2014, pg. 1080-1087

III.
5G NETWORKS
5G network is very fast and reliable. The concept of hand held devices is going to be revolutionized with the advent
of 5G. Now all the services and applications are going to be accessed by single IP as telephony, gaming and many
other multimedia applications. As it is not a new thing in market and there are millions of users all over the world
who have experienced the wireless services wireless technology. It is not easy for them to shrink from using this
new 5G network technology. There is only need to make it accessible so that a common man can easily afford the
profitable packs offered by the companies so that 5G network could hold the authentic place. There is need to win
the customer trust to build fair long term relation to make a reliable position in the telecommunication field. To
complete with the preceding wireless technologies in the market 5G network has to tender something reliable
something more pioneering. All the features like telephony, camera, mp3 player, are coming in new mobile phone
models. 4G is providing all these utility in mobile phone. By seeing the features of 4G one can gets a rough idea
about what 5G Networks could offer. There is messenger, photo gallery, and multimedia applications that are also
going to be the part of 5G. There would be no difference between a PC and a mobile phone rather both would act
vice versa [3].
IV.
DESIGN OF 5G MOBILE NETWORK ARCHITECTURE
Figure 9 shows the system model that proposes design of network architecture for 5G mobile systems, which is allIP based model for wireless and mobile networks interoperability. The system consists of a user terminal (which has
a crucial role in the new architecture) and a number of independent, autonomous radio access technologies. Within
each of the terminals, each of the radio access technologies is seen as the IP link to the outside Internet world.
However, there should be different radio interface for each Radio Access Technology (RAT) in the mobile terminal.
For an example, if we want to have access to four different RATs, we need to have four different access - specific
interfaces in the mobile terminal, and to have all of them active at the same time, with aim to have this architecture
to be functional. [9].
The first two OSI levels (data-link and physical levels) are defining the radio access technologies through which
is provided access to the Internet with more or less QoS support mechanisms, which is further dependent upon the
access technology (e.g., 3G and WiMAX have explicit QoS support, while WLAN has not) . Then, over the OSI-1
and OSI-2 layers is the network layer, and this layer is IP (Internet Protocol) in today’s communication world, either
IPv4 or IPv6, regardless of the radio access technology. The purpose of IP is to ensure enough control data (in IP
header) for proper routing of IP packets belonging to a certain application connections - sessions between client
applications and servers somewhere on the Internet. Routing of packets should be carried out in accordance with
established policies of the user [9].

Meenal G. Kachhavay et al, International Journal of Computer Science and Mobile Computing, Vol.3 Issue.3, March- 2014, pg. 1080-1087

Meenal G. Kachhavay et al, International Journal of Computer Science and Mobile Computing, Vol.3 Issue.3, March- 2014, pg. 1080-1087

Application connections are realized between clients and servers in the Internet via sockets. Internet sockets are
endpoints for data communication flows. Each socket of the web is a unified and unique combination of local IP
address and appropriate local transport communications port, target IP address and target appropriate
communication port, and type of transport protocol. Considering that, the establishment of communication from end
to end between the client and server using the Internet protocol is necessary to raise the appropriate Internet socket
uniquely determined by the application of the client and the server. This means that in case of interoperability
between heterogeneous networks and for the vertical handover between the respective radio technologies, the local
IP address and destination IP address should be fixed and unchanged. Fixing of these two parameters should ensure
handover transparency to the Internet connection end-to-end, when there is a mobile user at least on one end of such
connection. In order to preserve the proper layout of the packets and to reduce or prevent packets losses, routing to
the target destination and vice versa should be uniquely and using the same path. Each radio access technology that
is available to the user in achieving connectivity with the relevant radio access is presented with appropriate IP
interface. Each IP interface in the terminal is characterized by its IP address and net mask and parameters associated
with the routing of IP packets across the network. In regular inter-system handover the change of access technology
(i.e., vertical handover) would mean changing the local IP address. Then, change of any of the parameters of the
socket means and change of the socket, that is, closing the socket and opening a new one. This means, ending the
connection and starting e new one. This approach is not-flexible, and it is based on today’s Internet communication.
In order to solve this deficiency we propose a new level that will take care of the abstraction levels of network
access technologies to higher layers of the protocol stack. This layer is crucial in the new architecture. To enable the
functions of the applied transparency and control or direct routing of packets through the most appropriate radio
access technology, in the proposed architecture we introduce a control system in the functional architecture of the
networks, which works in complete coordination with the user terminal and provides a network abstraction functions
and routing of packets based on defined policies. At the same time this control system is an essential element
through which it can determine the quality of service for each transmission technology. He is on the Internet side of
the proposed architecture, and as such represents an ideal system to test the qualitative characteristics of the access
technologies, as well as to obtain a realistic picture regarding the quality that can be expected from applications of
the user towards a given server in Internet (or peer). Protocol setup of the new levels within the existing protocol
stack, which form the proposed architecture, is presented in Figure 2. The network abstraction level would be
provided by creating IP tunnels over IP interfaces obtained by connection to the terminal via the access technologies
available to the terminal (i.e., mobile user). In fact, the tunnels would be established between the user terminal and
control system named here as Policy Router, which performs routing based on given policies. In this way the client
side will create an appropriate number of tunnels connected to the number of radio access technologies, and the
client will only set a local IP address which will be formed with sockets Internet communication of client
applications with Internet servers. The way IP packets are routed through tunnels, or choosing the right tunnel,
would be served by policies whose rules will be exchanged via the virtual network layer protocol. This way we
achieve the required abstraction of the network to the client applications at the mobile terminal. The process of
establishing a tunnel to the Policy Router, for routing based on the policies, are carried out immediately after the
establishment of IP connectivity across the radio access technology, and it is initiated from the mobile terminal
Virtual Network-level Protocol. Establishing tunnel connections as well as maintaining them represents basic
functionality of the virtual network level (or network level of abstraction)[9].
V.
WHY IS 5G REQUIRED?
The major difference, from a user point of view, between current generations and expected 5G techniques must be
something else than increased maximum throughput; other requirements include:
•Lower out age probability; better coverage and high data rates available at cell edge.
•Lower battery consumption.
•Multiple concurrent data transfer paths.
•Around 1Gbps data rate in mobility.
•More secure; better cognitive radio/SDR Security.
•Higher system level spectral efficiency.
•World Wide wireless web (WWWW).
•More applications combined with artificial intelligent (AI) as human life will be surrounded by artificial sensors
which could be communicating with mobile phones. Not harmful to human health.
•Cheaper traffic fees due to low infra structure deployment costs[3].

Meenal G. Kachhavay et al, International Journal of Computer Science and Mobile Computing, Vol.3 Issue.3, March- 2014, pg. 1080-1087

VI.
CHARACTERISTICS OF 5G TECHNOLOGY
• The technology 5G presents the high resolution for sharp, passionate cell phone every day and give consumers
well shape and fast Internet access.
• The 5G technology provides billing limits in advance that the more beautiful and successful of the modern era.
• The 5G technology also allows users of mobile phones, cell phone records for printing
operations.
• The 5G technology for large volume data distribution in Gigabit, which also maintains close ties to almost 65,000.
• The technology gives you 5G carrier distribution gateways to unprecedented maximum stability without delay.
• The information from the data transfer technology 5G organize a more accurate and reliable results.
• Using remote control technology to get the consumer can also get a 5G comfort and relax by having a better speed
and clarity in less time alone.
• The 5G technology also support virtual private network.
• The uploading and downloading speed of 5G technology touching the peak.
• The 5G technology network offering enhanced and available connectivity just about the world.
• 5G network is very fast and reliable.
VII.
APPLICATIONS OF 5G TECHNOLOGY
1) Real wireless world with no more limitation with access and zone issues.
2) Wearable devices with AI capabilities.
3) Internet protocol version 6(IPv6), where a visiting care-of mobile IP address is assigned according to location and
connected network.
4) One unified global standard.
5) Pervasive networks providing ubiquitous computing: The user can simultaneously be connected to several
wireless access technologies and seamlessly move between them these access technologies can be a 2.5G,3G, 4G or
5G mobile networks, Wi-Fi, WPAN or any other future access technology. In 5G, the concept may be further
developed into multiple concurrent data transfer paths.
6) Cognitive radio technology, also known as smart radio: allowing different radio technologies to share the same
spectrum efficiently by adaptively finding unused spectrum and adapting the transmission scheme to the
requirements of the technologies currently sharing the spectrum. This dynamic radio resource management is
achieved in a distributed fashion, and relies on software defined radio.
7) High altitude stratospheric platform station (HAPS) Systems. The radio interface of 5G communication systems
is suggested in a Korean research and development program to be based on beam division multiple access (BDMA)
and group cooperative relay techniques.[8]
VIII. FUTURE SCOPE
Beyond 5g:
The future enhancement of Nano-core will be incredible as it combines with artificial intelligent (AI).One can able
to control his intelligent Robot using his mobile phone. Your Mobile can automatically type the message what your
brain thinks. We might get a circumstance where we don’t require any spectrum for communication. The Google hot
trends have rated the term 6G as the 17th most searched word in the search engines. The iPod 6G comes in seven
different colors and has an aluminum body which makes the body strong to with stand constant daily usage. It has a
clip on design like iPod shuffle and it attached to shirt firmly. 6G technology haven’t been fully revealed yet but
search phrases like what is 6G mobile technology, 6G technology, 6G mobile, 6G network, 6G wiki, 6G technology
ppt. are getting more familiar with new mobile technology getting evolved.
IX.
CONCLUSION
In this paper, we conclude that 5G network is very fast and reliable. Fifth generation is based on 4G technologies.
The 5th wireless mobile internet networks are real wireless world which shall be supported by LAS-CDMA (Large
Area Synchronized Code-Division Multiple Access),OFDM (Orthogonal frequency-division multiplexing),
MCCDMA(Multi-Carrier Code Division Multiple Access), UWB(Ultra-wideband), Network-LMDS( Local
Multipoint Distribution Service), and IPv6. Fifth generation technologies offers tremendous data capabilities and
unrestricted call volumes and infinite data broadcast together within latest mobile operating system. Fifth generation
should make an important difference and add more services and benefits to the world over 4G. Fifth generation
should be more intelligent technology that interconnects the entire world without limits. This generation is expected
to be released around 2020. The world of universal, uninterrupted access to information, entertainment and
communication will open new dimension to our lives and change our life style significantly.

Meenal G. Kachhavay et al, International Journal of Computer Science and Mobile Computing, Vol.3 Issue.3, March- 2014, pg. 1080-1087

REFERENCES
1. Aleksandar Tudzarov and Toni Janevski, “Functional Architecture for 5G Mobile Networks” International
Journal of Advanced Science and Technology Vol. 32, July, 2011.
2. Ms. Neha Dumbre, Ms. Monali Patwa, Ms. Kajal Patwa, “5G WIRELESS TECHNOLOGIES-Still 4G
auction not over, but time to start talking 5G” International Journal of Science, Engineering and
Technology Research (IJSETR) Volume 2, Issue 2, February 2013.
3. Akhilesh Kumar Pachauri and Ompal Singh, “5G Technology – Redefining wireless Communication in
upcoming Years” International Journal of Computer Science and Management Research Vol 1 Issue 1 Aug
2012 ISSN 2278 – 733X.
4. Ms. Reshma S. Sapakal, Ms. Sonali S. Kadam, “5G Mobile Technology” International Journal of
Advanced Research in Computer Engineering & Technology (IJARCET) Volume 2, Issue 2, February
2013.
5. Suvarna Patil, Vipin Patil, .Pallavi Bhatt, “A Review on 5G Technology” International Journal of
Engineering and Innovative Technology (IJEIT) Volume 1, Issue 1,January 2012.
6. Professor T.Venkat Narayana Rao, Aasha S. A. and Sravya Tirumalaraju, “5G TECHNOLOGIES – AN
ANECDOTE OF NETWORK SERVICE FOR THE FUTURE” Volume 2, No. 7, July 2011 Journal of
Global Research in Computer Science.
7. Saddam Hossain, “5G Wireless Communication Systems” American Journal of Engineering Research
(AJER) e-ISSN: 2320-0847 p-ISSN: 2320-0936 Volume-02, Issue-10, pp-344-353 www.ajer.org."
"Iris Localization Based on Integro-Differential
Operator for Unconstrained Infrared Iris Images
Vineet Kumar, Abhijit Asati, Anu Gupta

Department of Electrical and Electronics Engineering
Birla Institute of Technology and Science Pilani, Pilani Campus, India-333031
{vineet, abhijit_asati, anug}@pilani.bits-pilani.ac.in

Abstract—Iris localization is an important step for high
accuracy iris recognition systems and it becomes difficult for iris
images captured in unconstrained environments. The proposed
method localizes irises in unconstrained infrared iris images
having non-ideal issues such as severe reflections, eyeglasses, low
contrast, low illumination and occlusions by eyebrow hair,
eyelids and eyelashes. In the proposed method, the iris image is
first preprocessed using morphological operation to remove
reflections and make it suitable for subsequent steps. The
proposed method detects pupil using Daugman’s integrodifferential operator (IDO) and iris’s outer boundary is detected
using proposed modified Daugman’s IDO. The proposed method
proposes a technique based on thresholding and morphological
operation to reduce the number of pixels on which the IDO is
applied for detecting pupil which improves the time performance
and accuracy as well. The method was tested with CASIA-IrisThousand, version 4.0 (CITHV4) iris database which contains
challenging images having non-ideal issues as described before.
The average accuracy of the proposed method is 99.3% and
average time cost per image is 1.86 seconds for CITHV4. The
proposed method shows improvement in both accuracy and time
when compared with some published state-of-the-art iris
localization methods in the literature.
Keywords—Iris localization; Iris segmentation; Integrodifferential operator; Iris recognition.

I.

INTRODUCTION

Iris recognition offers high accuracy in security applications
such as secured access to bank accounts at ATM machines,
national border controls, secured access to buildings and
passports control etc. [1]. An iris recognition system [2,3,4]
typically contains four stages: iris localization, iris
normalization, feature extraction and template matching. The
iris localization is process of localizing iris’s inner and outer
boundaries. The iris localization is an important step because
inaccurate iris localization may result in different
representation of the iris patterns and such difference could
cause recognition failure [2]. Iris localization algorithm takes
input as an iris (eye) image from image acquisition system [4].
The iris images are captured under near infrared (NIR)
illuminations or visible wavelength (VW) light. Most of the
existing iris databases available for research purpose are NIR
databases because irises show rich and complex features in
NIR light [3]. The iris images collected in controlled
environments are better inputs for iris recognition algorithms
but the images captured in unconstrained environments may
have different non-ideal issues such as lighting and specular
reflections, eyeglasses, low contrast, low illuminations and
occlusions by eyebrows hair, eyelids and eyelashes [1] as
shown in Fig. 1. The unconstrained images complicate iris
localization process.

Most of the deployed and commercially available iris
recognition systems are typically using Daugman’s [3] and/or
Wildes’ [4] algorithms based iris localization. But the
algorithms in [3,4] work under very controlled conditions and
their performance deteriorates while dealing with the images
having non-ideal issues as discussed previously. Some other
published state-of-the-art iris localization methods are given in
[5-12] which localize irises in ideal and/or non-ideal images.
The proposed method presented in this paper localizes
irises in the unconstrained frontal view NIR iris images. In the
proposed method, image is preprocessed first to counter some
of non-ideal issues present in the image. Iris’s inner boundary
is localized by applying Daugman’s IDO on selected pixels in
the preprocessed image. The selected pixels are identified using
the image thresholding and morphological operation on binary
image. Iris’s outer boundary is detected using proposed IDO
which is a variant of Daugman’s IDO. The method was
implemented in MATLAB and tested with CASIA-IrisThousand, version 4.0 (CITHV4) iris database [13] which
contains NIR images having non-ideal issues as described
previously. The sample of unconstrained iris images from
CITHV4 are shown later in section IV of the paper.
Remainder of this paper is organized as follows: Related
Daugman’s IDO approach is discussed in Section II and
implementation of the proposed iris localization method is
explained in section III. Section IV explains experimental
results and discussion. Work is concluded in section V.
II. RELATED DAUGMAN’S IDO APPROACH
The review of iris localization methods in the relevant
literature reveals that there are two major approaches:
Daugman’s IDO and Hough transform based. As the proposed
method in this paper uses Daugman’s IDO as a circular edge
detector, so, Daugman’s IDO approach is discussed in this
section. The first successful iris localization method was
proposed by Daugman in 1993 which uses IDO (1) [3].
Daugman's IDO (1) was used to localize iris which considers
iris’s inner and outer boundaries as two circles inside iris
image [2,3]. He applied the IDO (1) to the image domain to
search for iris’s outer boundary first then within the iris’s
outer boundary to search for pupil [3]. Daugman applied it on
the images in which gray difference between iris and sclera is
more than pupil and iris [3]. This is true for images captured
under VW light. Given a pre-processed image, I(x,y), the IDO
(1) can be used first to determine the iris’s outer boundary.
Daugman’s IDO (1) is mathematically expressed as below [3].
The operator searches over image domain (x,y) for
maximum in the blurred partial derivative with respect to
increasing radius r of the normalized contour integral of I(x,y)
along a circular arc ds of radius r and center coordinates (xo,
yo) [3]. The symbol * denotes the convolution operation and
Gσ(r) is a smoothing function such as a Gaussian of scale σ
(standard deviation) [3]. To normalize the circular integral with
respect to its perimeter, it is divided by 2πr. In short, the IDO
(1) behaves as circular edge detector blurred at a scale set by σ,
which searches iteratively over image space through the
parameter set {xo, yo, r} [3]. First search is for iris’s outer
boundary with higher value of σ [3]. Once the iris’s outer
boundary is localized, the search process with finer value of σ,
for the iris inner boundary is carried out only within the predetermined region [3]. The computation time associated with
an iris’s outer boundary search process can be reduced by
providing a range of estimates for the parameter r, that are
close to the actual boundary radius.
III.

in the high intensity regions with the average of intensities of
pixels from the region surrounding them. The MATLAB
function ‘imfill’ with hole filling option available in [14] is
used as the morphological operator which is applied on the
complemented iris image. Taking the complement again,
returns the original iris image in which reflection spots and
high intensity pixels are removed as shown in Fig. 3(b). The
image shown in Fig. 3(b) is called preprocessed iris image
which is used as input image for both iris’s inner and outer
boundary localization.

THE PROPOSED IRIS LOCALIZATION METHOD

The pupil is localized prior to iris’s outer boundary because
pupil contour is stronger than iris’s outer boundary in NIR iris
images. The proposed method considers iris’s inner and outer
boundaries as perfect circles which is true for frontal view iris
images. The iris localization is achieved in three phases. In first
phase, iris image is preprocessed to counter some of the nonideal image issues and make it suitable for subsequent steps. In
second and third phase, iris’s inner and iris’s outer boundaries
are localized respectively. The steps involved in the proposed
iris localization method are shown in Fig. 2. The steps are
discussed in three phases of the method below.
A. Image preprocessing
The way IDO (1) works, it cannot be tolerant to reflection
spots and uneven high intensity pixels present in the iris image.
The reflection spots in iris images cover portions of the image
that causes hindrance in the iris detection process. The original
pixel intensity values are replaced by the much higher intensity
values in the parts of the image, affected by light reflections as
shown in Fig. 3(a). Therefore, the original information of these
parts of the image has been lost. As apparent from Fig. 1 and
Fig. 3(a), there is much difference in intensity values between
reflection spots and surrounding dark pixels. In order to avoid
light reflection and uneven high intensity pixel values affecting
the iris detection, a morphological operator is used which fills

B. Iris’s inner boundary (pupil) localization
To localize pupil using IDO (1), the image pixels on which
the IDO is applied are identified first otherwise it may take
much large time in localizing pupil. These identified pixels are
the potential centers of pupil circle (pupil-centers). The inputs
given to the IDO (1) for pupil localization are a range of pupil
radii (rminp to rmaxp) and a set of image pixels which are the
potential pupil-centers. The set of image pixels on which the
IDO (1) is applied are obtained using the steps below which
reduces the false candidate pixels those cannot be potential
pupil-centers. Pupil localization steps are shown in Fig. 4.
1) Image binarization: The pupil pixels are obtained
from the iris image using thresholding operation [15] as pupil
is relatively much darker region as compared with the iris,
sclera and the skin. The global thresholding is used in which
appropriate threshold intensity value is selected using the iris

image histogram [16]. The thresholding operation on an
intensity image f(x,y) with global threshold T results in binary
image g(x,y).
otherwise
The T value is selected once for whole iris database and do
not need to be computed for each iris image. The binary iris
image, shown in Fig. 4(b), is obtained by applying thresholding
on the preprocessed iris image of Fig. 4(a). The black regions
in the binary iris image (see Fig. 4(b)) correspond to pupil,
eyelashes, eyelids, eyeglass frame and low illumination near
the image border. The regions other than pupil in the image are
noise and are false candidate pixels for pupil-center. So, they
are removed using morphological operation below.
2) Binary Image cleanup: The noise in the binarizedimage is removed using image erosion for black objects which
is a morphological operation [15]. The erosion operation uses
a structuring element of type disk because pupil is circular
region [15]. The disk radius value must be smaller than the
minimum pupil radius in the selected iris image database. The
disk radius is taken as 15 for CITHV4. A larger disk radius
value may remove pupil completely and a smaller value may
not remove false candidate pixels significantly. The image
after erosion operation is shown in Fig. 4(c) which removes
the noise to a much extent and reduces pupil size also. This
step not only reduces false candidate pixels for pupil-center
due to noise but also removes pixels in the pupil which cannot
be potential pupil-center pixels. The false candidate pixels are
further reduced using step below.
3) Removing pixels close to image border: It is certain in
every iris image that pupil cannot be touching the image border
as pupil is surrounded by iris region. So, some pixels closer to
the image border can be discarded as they cannot be potential
pupil-centers. The range (distance) of discarded pixels from
image border is taken as {k×rminp}, where the k is a positive
scalar greater than one whose value is estimated by visualizing
the iris database images. The value of k chosen for CITHV4 is
3.5 and rminp is equal to 20. The value of k depends on the
database chosen. The remaining pixels after removing the
pixels close to border are shown in Fig. 4(d). The black pixels
in Fig. 4(d) are identified as potential pupil-centers.
The reduced candidate pixels as a result of above discussed
steps are identified as potential pupil-centers. Now, the IDO (1)
discussed in section II of the paper is applied on set of reduced
candidate pixels’ coordinates in the preprocessed iris image
which gives center and radius of pupil as output.
C. Iris’s outer boundary localization
Iris’s outer boundary localization may be hurdled by
eyelids, eyelashes, reflections and low contrast between the
iris and sclera. The reflections and uneven high intensity
values have already been removed after the image
preprocessing step. The eyelids and eyelashes occlusion as
shown in Fig. 5 is handled by IDO (2) which acts as arc
detector and it has been derived by taking motivation from
Daugman’s IDO (1).
The parameter r denotes the radius of the circular arc ds
centered at (xo,yo). To normalize the arc-pixels’ intensities
integral with respect to arc length, it is divided by 5 /12.The
IDO (2) searches iteratively over a portion of the image
domain for the maximum difference of sum of contour pixels
intensities between two adjacent circular arcs one pixel radius
apart and defined by {–π/4: π/6 and 5π/6: 5π/4} rad (see Fig.
5).

279

parts are always visible [10] which is used as the input
information to the IDO (2).
1) Define iris search region: The iris search area is
defined by the pupil center, radii range of the iris’s outer
boundary and a small area around pupil-center which contains
potential candidate pixels for centers of iris’s outer boundary
circle. The white area on both sides of pupil in Fig. 5
represents search region for the proposed IDO (2) with (xo,yo)
equal to pupil center. As the pupil and iris’s outer contour
circles may not be concentric [2], the IDO (2) is applied on a
rectangle of size 10 10 centered at pupil center to find
precise center and radius of iris’s outer boundary circle. Iris
localized iris image is shown in Fig. 6(b).

IV.

A. Input dataset
The short description of iris database [13] taken for testing
the proposed method is given below.


CITHV4 contains 20000 iris images collected from
1000 subjects. The main sources of intra-class
variations in the database are eyeglasses and specular
reflections [13]. CITHV4 images are 8-bit gray-level
JPEG files with resolution 640×480 pixel and collected
under near infrared illumination [13]. The CASIA-IrisThousand is the first publicly available iris dataset with
one thousand subjects. The detailed description is
available at [13]. Collectively, the images in the
database contain non-ideal issues such as occlusions by
eyelids, eyelashes, eyebrow hair, eyeglasses, low
illumination, low contrast and reflections.

The first 1000 images from CASIA Iris-Thousand V4.0
were taken for testing the proposed method.
B. Results and discussion
Fig. 7 shows sample of unconstrained images from the
CITHV4 in which accurately localized irises by the proposed
method are shown.

where NI is number of irises accurately localized and NT is
total number of iris images taken for testing. To find the
average time cost per image, first 1000 images from the
CITHV4 were taken. MATLAB timer functions ‘tic’ and ‘toc’
were used to find execution time of a code which runs to
localize 1000 images and dividing the execution time by 1000
gives average time cost per image. The experimental results of
the proposed localization method are shown in Table I.

EXPERIMENTAL RESULTS AND DISCUSSION

To evaluate performance of the proposed method, we used
MATLAB version 8.3 installed on a PC with Windows 7
Professional, Intel® Core™ i5 CPU @ 2.40 GHz, 8.00 GB
RAM and a set of images from CITHV4 [13].

TABLE I.

EXPERIMENTAL RESULTS

Accuracy (%) & Average time cost per image (sec)
Iris Database
CITHV4*
(640 480)

Proposed method

L. Masek method [17]

99.3 & 1.86

84 & 5.52

*CITHV4: CASIA-Iris-Thousand, version 4.0

For the same database images, we also tested L. Masek’s
iris localization method which is an edge detection and Hough
transform based algorithm whose open source code in
MATLAB is available on web at [17] and results of testing are
shown in Table I. Table I shows that the proposed method has
much better results than L. Masek method [17].
To compare the proposed method with published iris
localization methods in the literature, we chose Jan et al. [9,16]
work because their iris localization methods were tested with
the CITHV4 which we have used for testing the proposed
method. Also, Jan et al. [9,16] are highly accurate methods in
the literature as described in Jan et al. [9]. Table II shows that
accuracy of the proposed method is similar to Jan et al. [9,16]
but time cost performance is much better than Jan et al. [9,16]
methods. The major reason of reduced time cost of the
proposed method is reduced number of pixels on which IDO
(1) is applied during pupil localization. The use of IDO (2) also
reduces time cost of iris localization.
To compare the proposed method with Daugman’s
algorithm [2] and other famous published methods in the
literature [12,18], we also tested the proposed method with

MMU V1.0 iris database [19] because testing results for
CITHV4 were not found in the literature. Table III shows that
the accuracy of the proposed method for MMU V1.0 is 99.11%
which is better than some state-of-the-art published methods
[2,12,18]. The proposed method has average time cost of 0.67
seconds per image for MMU V1.0.
We also implemented Daugman’s algorithm described in
[2] which uses the IDO (1) to localize both boundaries of iris
and obtained accuracy of 93% for MMU V1.0. The use of IDO
(2) to localize iris’s outer boundary is major reason for
improvement in the accuracy of proposed method as compared
to Daugman’s algorithm [2]. The reduction in the number of
false candidate pixels that cannot be potential centers of pupil
circle is another factor that makes proposed method more
accurate.
TABLE II.

COMPARISON OF ACCURACY AND TIME COST WITH
PUBLISHED IRIS LOCALIZATION METHODS

ACKNOWLEDGMENT
Portions of the research in this paper use the CASIA-IrisV4
collected by the Chinese Academy of Sciences’ Institute of
Automation (CASIA). We thankfully acknowledge CASIA for
providing us the iris images. We also thank Multimedia
University for providing MMU V1.0.
REFERENCES
[1]
[2]
[3]
[4]
[5]

CITHV4*

Method

Accuracy (%) Time cost per image (sec)
Jan et al. [9]
99.5
4.93
Jan et al. [16]
99.23
3.4
Proposed method
99.3
1.86
*CITHV4: CASIA-Iris-Thousand, version 4.0

TABLE III.
COMPARISON OF ACCURACY WITH PUBLISHED RESULTS
(Accuracy results in the table are taken from [20])
Method
Accuracy (%) for MMU V1.0
Daugman [2]
Ma et al.[18]
Daugman (new)[12]
Proposed method

85.64
91.02
98.23
99.11

V. CONCLUSIONS
In this paper, we have presented a method to localize
irises in unconstrained NIR iris images. The proposed method
starts with image preprocessing followed by iris’s inner and
outer boundary detection using Daugman’s IDO and the
proposed modified Daugman’s IDO respectively. A novel
technique in pupil localization is proposed to reduce the
number of false candidate pixels which cannot be potential
centers of pupil circle. Also, the method proposes modified
Daugman’s IDO to detect iris’s outer boundary. The
introduction of above mentioned techniques enhance the time
and accuracy performance of iris localization based on the
IDO as demonstrated by testing results of the proposed
method. The experimental results show that the proposed
method is highly accurate and is tolerant to non-ideal issues in
iris images such reflections, eye-glasses, low contrast, low
illumination and occlusions by eyelids, eyelashes and eyebrow
hair. The comparison of the proposed method with published
work in the literature shows it outperforms on some state-ofthe-art iris localization methods in both accuracy and
computation time.

F. Jan, I. Usman, S.A. Khan, S.A. Malik, “A dynamic non-circular iris
localization technique for non-ideal data”, Comput. Electr. Eng., 2014,
http://dx.doi.org/10.1016/j.compeleceng.2014.05.004.
J. Daugman, “How iris recognition works”, IEEE Trans. Circuits Syst.
Video Technol., 14, (1), pp. 21–30, 2004.
J. Daugman, “High Confidence visual recognition of persons by a test of
statistical independence”, IEEE Trans. Pattern Anal. Mach. Intell., vol.
15, no. 11, pp. 1148–1160, Nov. 1993.
R.P. Wildes, “Iris recognition: An emerging biometric technology”,
Proc. IEEE, vol. 85, no. 9, pp. 1348–1363, Sep. 1997.
Z. He, T. Tan, Z. Sun, and X. Qiu, “Toward accurate and fast iris
segmentation for iris biometrics”, IEEE Trans. Pattern Anal. Mach.
Intell., vol. 31, no. 9, pp. 1670–1684, Jul. 2009.
Y. Chen, M. Adjouadi, C. Han et al., “A highly accurate and
computationally efficient approach for unconstrained iris segmentation”,
Image Vis. Comput., 28, (2), pp. 261–269, 2010.
T. Tan, Z. He, Z. Sun, “Efficient and robust segmentation of noisy iris
images for non-cooperative iris recognition”, Image Vis. Comput., 28,
(2), pp. 223–230, 2010.
N.B. Puhan, N. Sudha, A. Sivaraman Kaushalram, “Efficient
segmentation technique for noisy frontal view iris images using Fourier
spectral density”, Signal Image Video Process., 5, (1), pp. 105–119,
2011.
F. Jan, I. Usman, S. Agha, “Iris localization in frontal eye images for
less constrained iris recognition systems”, Digit. Signal Process., 22, (6),
pp. 971–986, 2012.
A. Radman, “Fast and reliable iris segmentation algorithm”, IET Image
Processing, 7(1), pp.42-49, 2013.
K. Grabowski, W. Sankowski, M. Zubert, M. Napieralska, “Reliable iris
localization method with application to iris recognition in near infrared
light”, MIXDES, Proceedings of the International Conference, June 2224,pp.684-687, 2006.
J. Daugman, New methods in iris recognition, IEEE Trans. Syst. Man
Cybern. B, Cybern., 2007, 37, (5), pp. 1167–1175.
CASIA Iris Image Database, 2010, Chinese Academy of Sciences’
Institute of Automation. Available: http://biometrics.idealtest.org/.
The MATLAB (version 8.3) function ‘imfill’ description, 2014.
Available: http://in.mathworks.com/help/images/ref/imfill.html.
R.C. Gonzalez, R.E. Woods, S.L. Eddins, Digital Image Processing
Using MATLAB, Gatesmark Publishing, 2009.
F. Jan, I. Usman, S. Agha, “Reliable iris localization using Hough
transform, histogram-bisection, and eccentricity”, Signal Processing,
93(1), pp. 230–241, 2013.
L. Masek, P. Kovesi, “MATLAB source code for a biometric
identification system based on iris patterns”, The School of Computer
Science and Software Engineering, The University of Western Australia,
2003. Available: http://www.csse.uwa.edu.au/~pk/studentprojects/libor/
sourcecode.html.
L. Ma, T. Tan, Y. Wang, D. Zhang, “Local intensity variation analysis
for iris recognition”, Pattern Recognition, vol. 37, Issue 6, pp. 1287–98,
2004
MMU Iris Image Database, 2004, Multimedia University. Available:
http://pesona. mmu.edu. my/~ccteo/.
M.T. Ibrahim, T.M. Khan, S.A. Khan, M.A. Khan, L. Guan, “Iris
localization using local histogram and other image statistics”, Opt.
Lasers Eng., 50, (5), pp. 645–654, 2012."
"Internet of Things for Smart Cities
Andrea Zanella, Senior Member, IEEE, Nicola Bui, Angelo Castellani,
Lorenzo Vangelista, Senior Member, IEEE, and Michele Zorzi, Fellow, IEEE

Abstract—The Internet of Things (IoT) shall be able to incorporate transparently and seamlessly a large number of different and
heterogeneous end systems, while providing open access to selected
subsets of data for the development of a plethora of digital services.
Building a general architecture for the IoT is hence a very complex
task, mainly because of the extremely large variety of devices, link
layer technologies, and services that may be involved in such a
system. In this paper, we focus speciﬁcally to an urban IoT system
that, while still being quite a broad category, are characterized by
their speciﬁc application domain. Urban IoTs, in fact, are designed
to support the Smart City vision, which aims at exploiting the most
advanced communication technologies to support added-value
services for the administration of the city and for the citizens. This
paper hence provides a comprehensive survey of the enabling
technologies, protocols, and architecture for an urban IoT. Furthermore, the paper will present and discuss the technical solutions
and best-practice guidelines adopted in the Padova Smart City
project, a proof-of-concept deployment of an IoT island in the
city of Padova, Italy, performed in collaboration with the city
municipality.
Index Terms—Constrained Application Protocol (CoAP),
Efﬁcient XML Interchange (EXI), network architecture, sensor
system integration, service functions and management, Smart
Cities, testbed and trials, 6lowPAN.

I. INTRODUCTION
HE Internet of Things (IoT) is a recent communication
paradigm that envisions a near future, in which the objects
of everyday life will be equipped with microcontrollers, transceivers for digital communication, and suitable protocol stacks
that will make them able to communicate with one another and
with the users, becoming an integral part of the Internet [1]. The
IoT concept, hence, aims at making the Internet even more
immersive and pervasive. Furthermore, by enabling easy access
and interaction with a wide variety of devices such as, for
instance, home appliances, surveillance cameras, monitoring
sensors, actuators, displays, vehicles, and so on, the IoT will
foster the development of a number of applications that make use
of the potentially enormous amount and variety of data generated
by such objects to provide new services to citizens, companies,
and public administrations. This paradigm indeed ﬁnds application in many different domains, such as home automation,
industrial automation, medical aids, mobile healthcare, elderly
assistance, intelligent energy management and smart grids,
automotive, trafﬁc management, and many others [2].
However, such a heterogeneous ﬁeld of application makes the
identiﬁcation of solutions capable of satisfying the requirements
of all possible application scenarios a formidable challenge. This
difﬁculty has led to the proliferation of different and, sometimes,
incompatible proposals for the practical realization of IoT systems. Therefore, from a system perspective, the realization of an
IoT network, together with the required backend network services and devices, still lacks an established best practice because
of its novelty and complexity. In addition to the technical
difﬁculties, the adoption of the IoT paradigm is also hindered
by the lack of a clear and widely accepted business model that
can attract investments to promote the deployment of these
technologies [3].
In this complex scenario, the application of the IoT paradigm
to an urban context is of particular interest, as it responds to the
strong push of many national governments to adopt ICT solutions in the management of public affairs, thus realizing the
so-called Smart City concept [4]. Although there is not yet a
formal and widely accepted deﬁnition of “Smart City,” the ﬁnal
aim is to make a better use of the public resources, increasing the
quality of the services offered to the citizens, while reducing the
operational costs of the public administrations. This objective
can be pursued by the deployment of an urban IoT, i.e., a
communication infrastructure that provides uniﬁed, simple, and
economical access to a plethora of public services, thus unleashing potential synergies and increasing transparency to the citizens. An urban IoT, indeed, may bring a number of beneﬁts in the
management and optimization of traditional public services,
such as transport and parking, lighting, surveillance and maintenance of public areas, preservation of cultural heritage, garbage
collection, salubrity of hospitals, and school.1 Furthermore, the
availability of different types of data, collected by a pervasive
urban IoT, may also be exploited to increase the transparency and
promote the actions of the local government toward the citizens,
enhance the awareness of people about the status of their city,
stimulate the active participation of the citizens in the management of public administration, and also stimulate the creation of
new services upon those provided by the IoT [5]. Therefore, the

application of the IoT paradigm to the Smart City is particularly
attractive to local and regional administrations that may become
the early adopters of such technologies, thus acting as catalyzers
for the adoption of the IoT paradigm on a wider scale.
The objective of this paper is to discuss a general reference
framework for the design of an urban IoT. We describe the
speciﬁc characteristics of an urban IoT, and the services that may
drive the adoption of urban IoT by local governments. We then
overview the web-based approach for the design of IoT services,
and the related protocols and technologies, discussing their
suitability for the Smart City environment. Finally, we substantiate the discussion by reporting our experience in the “Padova
Smart City” project, which is a proof-of-concept deployment of
an IoT island in the city of Padova (Italy) and interconnected with
the data network of the city municipality. In this regard, we
describe the technical solutions adopted for the realization of the
IoT island and report some of the measurements that have been
collected by the system in its ﬁrst operational days.
The rest of the paper is organized as follows. Section II
overviews the services that are commonly associated to the
Smart City vision and that can be enabled by the deployment
of an urban IoT. Section III provides a general overview of the
system architecture for an urban IoT. More in detail, this section
describes the web service approach for the realization of IoT
services, with the related data formats and communication
protocols, and the link layer technologies. Finally, Section IV
presents the “Padova Smart City” project, which exempliﬁes a
possible implementation of an urban IoT and provides examples
of the type of data that can be collected with such a structure.
II. SMART CITY CONCEPT AND SERVICES
According to Pike Research on Smart Cities,2 the Smart City
market is estimated at hundreds of billion dollars by 2020, with
an annual spending reaching nearly 16 billions. This market
springs from the synergic interconnection of key industry and

service sectors, such as Smart Governance, Smart Mobility,
Smart Utilities, Smart Buildings, and Smart Environment. These
sectors have also been considered in the European Smart Cities
project (http://www.smart-cities.eu) to deﬁne a ranking criterion
that can be used to assess the level of “smartness” of European
cities. Nonetheless, the Smart City market has not really taken off
yet, for a number of political, technical, and ﬁnancial barriers [6].
Under the political dimension, the primary obstacle is the
attribution of decision-making power to the different stakeholders. A possible way to remove this roadblock is to institutionalize the entire decision and execution process, concentrating
the strategic planning and management of the smart city aspects
into a single, dedicated department in the city [7].
On the technical side, the most relevant issue consists in the
noninteroperability of the heterogeneous technologies currently
used in city and urban developments. In this respect, the IoT
vision can become the building block to realize a uniﬁed urbanscale ICT platform, thus unleashing the potential of the Smart
City vision [8], [9].
Finally, concerning the ﬁnancial dimension, a clear business
model is still lacking, although some initiative to ﬁll this gap has
been recently undertaken [10]. The situation is worsened by the
adverse global economic situation, which has determined a
general shrinking of investments on public services. This situation prevents the potentially huge Smart City market from
becoming reality. A possible way out of this impasse is to ﬁrst
develop those services that conjugate social utility with very
clear return on investment, such as smart parking and smart
buildings, and will hence act as catalyzers for the other addedvalue services [10].
In the rest of this section, we overview some of the services
that might be enabled by an urban IoT paradigm and that are of
potential interest in the Smart City context because they can
realize the win–win situation of increasing the quality and
enhancing the services offered to the citizens while bringing an
economical advantage for the city administration in terms of
reduction of the operational costs [6]. To better appreciate the
level of maturity of the enabling technologies for these services,
we report in Table I a synoptic view of the services in terms of

suggested type(s) of network to be deployed, expected trafﬁc
generated by the service, maximum tolerable delay, device
powering, and an estimate of the feasibility of each service
with currently available technologies. From the table, it clearly
emerges that, in general, the practical realization of most
of such services is not hindered by technical issues, but rather
by the lack of a widely accepted communication and service
architecture that can abstract from the speciﬁc features of the
single technologies and provide harmonized access to the
services.
Structural Health of Buildings: Proper maintenance of the
historical buildings of a city requires the continuous monitoring
of the actual conditions of each building and identiﬁcation of
the areas that are most subject to the impact of external agents.
The urban IoT may provide a distributed database of building
structural integrity measurements, collected by suitable sensors
located in the buildings, such as vibration and deformation
sensors to monitor the building stress, atmospheric agent sensors
in the surrounding areas to monitor pollution levels, and temperature and humidity sensors to have a complete characterization of the environmental conditions [11]. This database should
reduce the need for expensive periodic structural testing by
human operators and will allow targeted and proactive maintenance and restoration actions. Finally, it will be possible to
combine vibration and seismic readings in order to better study
and understand the impact of light earthquakes on city buildings.
This database can be made publicly accessible in order to make
the citizens aware of the care taken in preserving the city
historical heritage. The practical realization of this service,
however, requires the installation of sensors in the buildings
and surrounding areas and their interconnection to a control
system, which may require an initial investment in order to create
the needed infrastructure.
Waste Management: Waste management is a primary issue in
many modern cities, due to both the cost of the service and the
problem of the storage of garbage in landﬁlls. A deeper penetration of ICT solutions in this domain, however, may result in
signiﬁcant savings and economical and ecological advantages.
For instance, the use of intelligent waste containers, which detect
the level of load and allow for an optimization of the collector
trucks route, can reduce the cost of waste collection and improve
the quality of recycling [12].3 To realize such a smart waste
management service, the IoT shall connect the end devices, i.e.,
intelligent waste containers, to a control center where an optimization software processes the data and determines the optimal
management of the collector truck ﬂeet.
Air Quality: The European Union ofﬁcially adopted a 20-2020 Renewable Energy Directive setting climate change reduction
goals for the next decade.4 The targets call for a 20% reduction in
greenhouse gas emissions by 2020 compared with 1990 levels, a

20% cut in energy consumption through improved energy
efﬁciency by 2020, and a 20% increase in the use of renewable
energy by 2020. To such an extent, an urban IoT can provide
means to monitor the quality of the air in crowded areas, parks, or
ﬁtness trails [13]. In addition, communication facilities can be
provided to let health applications running on joggers’ devices be
connected to the infrastructure. In such a way, people can always
ﬁnd the healthiest path for outdoor activities and can be continuously connected to their preferred personal training application.
The realization of such a service requires that air quality and
pollution sensors be deployed across the city and that the sensor
data be made publicly available to citizens.
Noise Monitoring: Noise can be seen as a form of acoustic
pollution as much as carbon oxide (CO) is for air. In that sense,
the city authorities have already issued speciﬁc laws to reduce the
amount of noise in the city centre at speciﬁc hours. An urban IoT
can offer a noise monitoring service to measure the amount of
noise produced at any given hour in the places that adopt the
service [14]. Besides building a space-time map of the noise
pollution in the area, such a service can also be used to enforce
public security, by means of sound detection algorithms that can
recognize, for instance, the noise of glass crashes or brawls. This
service can hence improve both the quiet of the nights in the city
and the conﬁdence of public establishment owners, although the
installation of sound detectors or environmental microphones is
quite controversial, because of the obvious privacy concerns for
this type of monitoring.
Trafﬁc Congestion: On the same line of air quality and noise
monitoring, a possible Smart City service that can be enabled by
urban IoT consists in monitoring the trafﬁc congestion in the city.
Even though camera-based trafﬁc monitoring systems are already
available and deployed in many cities, low-power widespread
communication can provide a denser source of information.
Trafﬁc monitoring may be realized by using the sensing capabilities and GPS installed on modern vehicles [15], and also adopting
a combination of air quality and acoustic sensors along a
given road. This information is of great importance for city
authorities and citizens: for the former to discipline trafﬁc and
to send ofﬁcers where needed and for the latter to plan in advance
the route to reach the ofﬁce or to better schedule a shopping trip to
the city centre.
City Energy Consumption: Together with the air quality
monitoring service, an urban IoT may provide a service to
monitor the energy consumption of the whole city, thus enabling
authorities and citizens to get a clear and detailed view of the
amount of energy required by the different services (public
lighting, transportation, trafﬁc lights, control cameras, heating/
cooling of public buildings, and so on). In turn, this will make it
possible to identify the main energy consumption sources and to
set priorities in order to optimize their behavior. This goes in the
direction indicated by the European directive for energy efﬁciency improvement in the next years. In order to obtain such a
service, power draw monitoring devices must be integrated with
the power grid in the city. In addition, it will also be possible to
enhance these service with active functionalities to control local
power production structures (e.g., photovoltaic panels).
Smart Parking: The smart parking service is based on road
sensors and intelligent displays that direct motorists along the

best path for parking in the city [16]. The beneﬁts deriving from
this service are manifold: faster time to locate a parking slot
means fewer CO emission from the car, lesser trafﬁc congestion,
and happier citizens. The smart parking service can be directly
integrated in the urban IoT infrastructure, because many
companies in Europe are providing market products for this
application. Furthermore, by using short-range communication
technologies, such as Radio Frequency Identiﬁers (RFID) or
Near Field Communication (NFC), it is possible to realize an
electronic veriﬁcation system of parking permits in slots reserved
for residents or disabled, thus offering a better service to citizens
that can legitimately use those slots and an efﬁcient tool to
quickly spot violations.
Smart Lighting: In order to support the 20-20-20 directive, the
optimization of the street lighting efﬁciency is an important
feature. In particular, this service can optimize the street lamp
intensity according to the time of the day, the weather condition,
and the presence of people. In order to properly work, such a
service needs to include the street lights into the Smart City
infrastructure. It is also possible to exploit the increased number
of connected spots to provide WiFi connection to citizens. In
addition, a fault detection system will be easily realized on top of
the street light controllers.
Automation and Salubrity of Public Buildings: Another
important application of IoT technologies is the monitoring of
the energy consumption and the salubrity of the environment in
public buildings (schools, administration ofﬁces, and museums)
by means of different types of sensors and actuators that control
lights, temperature, and humidity. By controlling these parameters, indeed, it is possible to enhance the level of comfort of the
persons that live in these environments, which may also have a
positive return in terms of productivity, while reducing the costs
for heating/cooling [17].
III. URBAN IOT ARCHITECTURE
From the analysis of the services described in Section II, it
clearly emerges that most Smart City services are based on a
centralized architecture, where a dense and heterogeneous set of
peripheral devices deployed over the urban area generate different types of data that are then delivered through suitable communication technologies to a control center, where data storage
and processing are performed.
A primary characteristic of an urban IoT infrastructure, hence,
is its capability of integrating different technologies with the
existing communication infrastructures in order to support a
progressive evolution of the IoT, with the interconnection of
other devices and the realization of novel functionalities and
services. Another fundamental aspect is the necessity to make
(part of) the data collected by the urban IoT easily accessible
by authorities and citizens, to increase the responsiveness of
authorities to city problems, and to promote the awareness and
the participation of citizens in public matters [9].
In the rest of this section, we describe the different components
of an urban IoT system, as sketched in Fig. 1. We start describing
the web service approach for the design of IoT services, which
requires the deployment of suitable protocol layers in the different elements of the network, as shown in the protocol stacks
depicted in Fig. 1, besides the key elements of the architecture.
Then, we brieﬂy overview the link layer technologies that can be
used to interconnect the different parts of the IoT. Finally, we
describe the heterogeneous set of devices that concur to the
realization of an urban IoT.
A. Web Service Approach for IoT Service Architecture
Although in the IoT domain many different standards are still
struggling to be the reference one and the most adopted, in this
section we focus speciﬁcally on IETF standards because they are
open and royalty-free, are based on Internet best practices, and
can count on a wide community.
The IETF standards for IoT embrace a web service architecture for IoT services, which has been widely documented in the
literature as a very promising and ﬂexible approach. In fact, web
services permit to realize a ﬂexible and interoperable system that
can be extended to IoT nodes, through the adoption of the webbased paradigm known as Representational State Transfer
(ReST) [18]. IoT services designed in accordance with the ReST
paradigm exhibit very strong similarity with traditional web
services, thus greatly facilitating the adoption and use of IoT
by both end users and service developers, which will be able to
easily reuse much of the knowledge gained from traditional web
technologies in the development of services for networks containing smart objects. The web service approach is also promoted
by international standardization bodies such as IETF, ETSI, and
W3C, among others, as well as European research projects on the
IoT such as SENSEI,5 IoT-A,6 and SmartSantander.1
Fig. 2 shows a reference protocol architecture for the urban IoT
system that entails both an unconstrained and a constrained
protocol stack. The ﬁrst consists of the protocols that are
currently the de-facto standards for Internet communications,
and are commonly used by regular Internet hosts, such as XML,
HTTP, and IPv4. These protocols are mirrored in the constrained
protocol stack by their low-complexity counterparts, i.e., the
Efﬁcient XML Interchange (EXI), the Constrained Application
Protocol (CoAP), and 6LoWPAN, which are suitable even for
very constrained devices. The transcoding operations between
the protocols in the left and right stacks in Fig. 2 can be performed
in a standard and low complexity manner, thus guaranteeing easy
access and interoperability of the IoT nodes with the Internet.
It may be worth remarking that systems that do not adopt the
EXI/CoAP/6LoWPAN protocol stack can still be seamlessly
included in the urban IoT system, provided that they are capable
of interfacing with all the layers of the left-hand side of the
protocol architecture in Fig. 2.
In the protocol architecture shown in Fig. 2, we can distinguish
three distinct functional layers, namely (i) Data, (ii) Application/
Transport, and (iii) Network, that may require dedicated entities
to operate the transcoding between constrained and unconstrained formats and protocols. In the rest of this section, we
specify in greater detail the requirements at each of the three
functional layers in order to guarantee interoperability among the
different parts of the system.
1) Data Format: As mentioned, the urban IoT paradigm sets
speciﬁc requirements in terms of data accessibility. In architectures based on web services, data exchange is typically
accompanied by a description of the transferred content by
means of semantic representation languages, of which the
eXtensible Markup Language (XML) is probably the most
common. Nevertheless, the size of XML messages is often
too large for the limited capacity of typical devices for the
IoT. Furthermore, the text nature of XML representation
makes the parsing of messages by CPU-limited devices more
complex compared to the binary formats. For these reasons, the
working group of the World Wide Web Consortium (W3C)7 has
proposed the EXI format [19], which makes it possible even for
very constrained devices to natively support and generate
messages using an open data format compatible with XML.
EXI deﬁnes two types of encoding, namely schema-less and
schema-informed. While the schema-less encoding is generated
directly from the XML data and can be decoded by any EXI
entity without any prior knowledge about the data, the schemainformed encoding assumes that the two EXI processors share an
7

XML Schema before actual encoding and decoding can take
place. This shared schema makes it possible to assign numeric
identiﬁers to the XML tags in the schema and build the EXI
grammars upon such coding. As discussed in [20], a general
purpose schema-informed EXI processor can be easily integrated
even in very constrained devices, enabling them to interpret EXI
formats and, hence, making it possible to build multipurpose IoT
nodes even out of very constrained devices. Using the schemainformed approach, however, requires additional care in the
development of higher layer application, since developers need
to deﬁne an XML Schema for the messages involved in the
application and use EXI processors that support this operating
mode. Further details about EXI and schema-informed processing can be found in [20].
Integration of multiple XML/EXI data sources into an IoT
system can be obtained by using the databases typically created
and maintained by high-level applications. In fact, IoT applications generally build a database of the nodes controlled by the
application and, often, of the data generated by such nodes. The
database makes it possible to integrate the data received by any
IoT device to provide the speciﬁc service the application is built
for. A generic framework for building IoT web applications
according to the guidelines described in this section has been
proposed in [21], where the authors also suggest exploiting the
Asynchronous JavaScript and XML (AJAX) capabilities of
modern web browsers that allow for a direct communication
between the browser and the ﬁnal IoT node, demonstrating the
full internetworking of the protocol stack and the open data
nature of the proposed approach.
2) Application and Transport Layers: Most of the trafﬁc that
crosses the Internet nowadays is carried at the application layer
by HTTP over TCP. However, the verbosity and complexity of
native HTTP make it unsuitable for a straight deployment on
constrained IoT devices. For such an environment, in fact, the
human-readable format of HTTP, which has been one of the
reasons of its success in traditional networks, turns out to be a
limiting factor due to the large amount of heavily correlated (and,
hence, redundant) data. Moreover, HTTP typically relies upon
the TCP transport protocol that, however, does not scale well on
constrained devices, yielding poor performance for small data
ﬂows in lossy environments.
The CoAP protocol [22] overcomes these difﬁculties by
proposing a binary format transported over UDP, handling only
the retransmissions strictly required to provide a reliable service.
Moreover, CoAP can easily interoperate with HTTP because:
(i) it supports the ReST methods of HTTP (GET, PUT, POST,
and DELETE), (ii) there is a one-to-one correspondence between
the response codes of the two protocols, and (iii) the CoAP
options can support a wide range of HTTP usage scenarios.
Even though regular Internet hosts can natively support
CoAP to directly talk to IoT devices, the most general and
easily interoperable solution requires the deployment of an
HTTP-CoAP intermediary, also known as cross proxy that can
straightforwardly translate requests/responses between the two
protocols, thus enabling transparent interoperation with native
HTTP devices and applications [23].
3) Network Layer: IPv4 is the leading addressing technology
supported by Internet hosts. However, IANA, the international

organization that assigns IP addresses at a global level, has
recently announced the exhaustion of IPv4 address blocks. IoT
networks, in turn, are expected to include billions of nodes, each of
which shall be (in principle) uniquely addressable. A solution to
this problem is offered by the IPv6 standard [24], which provides
a 128-bit address ﬁeld, thus making it possible to assign a unique
IPv6 address to any possible node in the IoT network.
While, on the one hand, the huge address space of IPv6 makes
it possible to solve the addressing issues in IoT; on the other
hand, it introduces overheads that are not compatible with the
scarce capabilities of constrained nodes. This problem can be
overcome by adopting 6LoWPAN [25], [26], which is an
established compression format for IPv6 and UDP headers over
low-power constrained networks. A border router, which is a
device directly attached to the 6LoWPAN network, transparently
performs the conversion between IPv6 and 6LoWPAN, translating any IPv6 packet intended for a node in the 6LoWPAN
network into a packet with 6LoWPAN header compression
format, and operating the inverse translation in the opposite
direction.
While the deployment of a 6LoWPAN border router enables
transparent interaction between IoT nodes and any IPv6 host in
the Internet, the interaction with IPv4-only hosts remains an
issue. More speciﬁcally, the problem consists in ﬁnding a way to
address a speciﬁc IPv6 host using an IPv4 address and other
meta-data available in the packet. In the following, we present
different approaches to achieve this goal.
v4/v6 Port Address Translation (v4/v6 PAT). This method
maps arbitrary pairs of IPv4 addresses and TCP/UDP ports into
IPv6 addresses and TCP/UDP ports. It resembles the classical
Network Address and Port Translation (NAPT) service currently
supported in many LANs to provide Internet access to a number
of hosts in a private network by sharing a common public IPv4
address, which is used to address the packets over the public
Internet. When a packet is returned to the IPv4 common address,
the edge router that supports the NATP service will intercept the
packet and replace the common IPv4 destination address with the
(private) address of the intended receiver, which is determined by
looking up in the NATP table the address of the host associated to
the speciﬁc destination port carried by the packet. The same
technique can be used to map multiple IPv6 addresses into a
single IPv4 public address, which allows the forwarding of the
datagrams in the IPv4 network and its correct management at
IPv4-only hosts. The application of this technique requires low
complexity and, indeed, port mapping is an established technique for v4/v6 transition. On the other hand, this approach raises
a scalability problem, since the number of IPv6 hosts that can be
multiplexed into a single IPv4 address is limited by the number of
available TCP/UDP ports (65535). Furthermore, this approach
requires that the connection be initiated by the IPv6 nodes in
order to create the correct entries in the NATP look-up table.
Connections starting from the IPv4 cloud can also be realized,
but this requires a more complex architecture, with the local DNS
placed within the IPv6 network and statically associated to a
public IPv4 address in the NATP translation table.
v4/v6 Domain Name Conversion. This method, originally
proposed in [23], is similar to the technique used to provide
virtual hosting service in HTTP 1.1, which makes it possible to
support multiple websites on the same web server, sharing the
same IPv4 address, by exploiting the information contained in
the HTTP Host header to identify the speciﬁc web site requested
by the user. Similarly, it is possible to program the DNS servers in
such a way that, upon a DNS request for the domain name of an
IoT web service, the DNS returns the IPv4 address of an HTTPCoAP cross proxy to be contacted to access the IoT node. Once
addressed by an HTTP request, the proxy requires the resolution
of the domain name contained in the HTTP Host header to the
IPv6 DNS server, which replies with the IPv6 address that
identiﬁes the ﬁnal IoT node involved in the request. The proxy
can then forward the HTTP message to the intended IoT via
CoAP.
URI mapping. The Universal Resource Identiﬁer (URI) mapping technique is also described in [23]. This technique involves
a particular type of HTTP-CoAP cross proxy, the reverse cross
proxy. This proxy behaves as being the ﬁnal web server to the
HTTP/IPv4 client and as the original client to the CoAP/IPv6
web server. Since this machine needs to be placed in a part of the
network where IPv6 connectivity is present to allow direct access
to the ﬁnal IoT nodes, IPv4/IPv6 conversion is internally
resolved by the applied URI mapping function.
B. Link Layer Technologies
An urban IoT system, due to its inherently large deployment
area, requires a set of link layer technologies that can easily cover
a wide geographical area and, at the same time, support a possibly
large amount of trafﬁc resulting from the aggregation of an
extremely high number of smaller data ﬂows. For these reasons,
link layer technologies enabling the realization of an urban IoT
system are classiﬁed into unconstrained and constrained technologies. The ﬁrst group includes all the traditional LAN, MAN,
and WAN communication technologies, such as Ethernet, WiFi,
ﬁber optic, broadband Power Line Communication (PLC), and
cellular technologies such as UMTS and LTE. They are generally
characterized by high reliability, low latency, and high transfer
rates (order of Mbit/s or higher), and due to their inherent
complexity and energy consumption are generally not suitable
for peripheral IoT nodes.
The constrained physical and link layer technologies are,
instead, generally characterized by low energy consumption and
relatively low transfer rates, typically smaller than 1 Mbit/s. The
more prominent solutions in this category are IEEE 802.15.4
[27], [28] Bluetooth and Bluetooth Low Energy,8 IEEE 802.11
Low Power, PLC [29], NFC and RFID [30]. These links usually
exhibit long latencies, mainly due to two factors: 1) the intrinsically low transmission rate at the physical layer and 2) the powersaving policies implemented by the nodes to save energy, which
usually involve duty cycling with short active periods.
C. Devices
We ﬁnally describe the devices that are essential to realize an
urban IoT, classiﬁed based on the position they occupy in the
communication ﬂow.

1) Backend Servers: At the root of the system, we ﬁnd the
backend servers, located in the control center, where data are
collected, stored, and processed to produce added-value services.
In principle, backend servers are not mandatory for an IoT system to properly operate, though they become a fundamental
component of an urban IoT where they can facilitate the access to
the smart city services and open data through the legacy network
infrastructure. Backend systems commonly considered for
interfacing with the IoT data feeders include the following.
Database management systems: These systems are in
charge of storing the large amount of information produced by
IoT peripheral nodes, such as sensors. Depending on the particular usage scenario, the load on these systems can be quite large,
so that proper dimensioning of the backend system is required.
Web sites: The widespread acquaintance of people with web
interfaces makes them the ﬁrst option to enable interoperation
between the IoT system and the “data consumers,” e.g., public
authorities, service operators, utility providers, and common
citizens.
Enterprise resource planning systems (ERP): ERP components support a variety of business functions and are precious
tools to manage the ﬂow of information across a complex
organization, such as a city administration. Interfacing ERP
components with database management systems that collect the
data generated by the IoT allows for a simpler management of
the potentially massive amount of data gathered by the IoT,
making it possible to separate the information ﬂows based on
their nature and relevance and easing the creation of new
services.
2) Gateways: Moving toward the “edge” of the IoT, we ﬁnd
the gateways, whose role is to interconnect the end devices
to the main communication infrastructure of the system. With
reference to the conceptual protocol architecture depicted in
Fig. 2, the gateway is hence required to provide protocol
translation and functional mapping between the unconstrained
protocols and their constrained counterparts, that is to say XMLEXI, HTTP-CoAP, IPv4/v6-6LoWPAN.
Note that while all these translations may be required in order
to enable interoperability with IoT peripheral devices and control
stations, it is not necessary to concentrate all of them in a single
gateway. Rather, it is possible, and sometimes convenient, to
distribute the translation tasks over different devices in the
network. For example, a single HTTP-CoAP proxy can be
deployed to support multiple 6LoWPAN border routers.
Gateway devices shall also provide the interconnection between unconstrained link layer technologies, mainly used in the
core of the IoT network, and constrained technologies that,
instead, provide connectivity among the IoT peripheral nodes.
3) IoT Peripheral Nodes: Finally, at the periphery of the IoT
system, we ﬁnd the devices in charge of producing the data
to be delivered to the control center, which are usually called
IoT peripheral nodes or, more simply, IoT nodes. Generally
speaking, the cost of these devices is very low, starting from
10 USD or even less, depending on the kind and number of
sensors/actuators mounted on the board. IoT nodes may be
classiﬁed based on a wide number of characteristics, such as
powering mode, networking role (relay or leaf), sensor/actuator
equipment, and supported link layer technologies. The most
constrained IoT nodes are likely the Radio Frequency tags
(RFtags) that, despite their very limited capabilities, can still
play an important role in IoT systems, mainly because of the
extremely low cost and the passive nature of their communication hardware, which does not require any internal energy
source. The typical application of RFtags is object identiﬁcation
by proximity reading, which can be used for logistics,
maintenance, monitoring, and other services.
Mobile devices, such as smart phones, tablet PCs, or laptops,
may also be an important part of an urban IoT, providing other
ways to interact with it. For instance, the NFC transceiver
integrated in last-generation smartphones may be used to identify
tagged objects, while the geolocation service provided by most
common operating systems for mobile devices can enrich the
context information associated to that object. Furthermore,
mobile devices can provide access to the IoT in different ways,
e.g., 1) through an IP connection provided by the cellular datalink service or 2) setting up a direct connection with some objects
by using short-range wireless technologies, such as Bluetooth
Low Energy, low-power WiFi, or IEEE 802.15.4. Furthermore, it
is possible to develop speciﬁc applications for mobile devices
that can ease the interaction with the IoT objects, and with the
system as a whole.
IV. AN"
"I. INTRODUCTION
HE INTERNET of Things (IoT) is regarded as a
technology and economic wave in the global information
industry after the Internet. The IoT is an intelligent network
which connects all things to the Internet for the purpose
of exchanging information and communicating through the
information sensing devices in accordance with agreed protocols. It achieves the goal of intelligent identifying, locating,
tracking, monitoring, and managing things [1]. It is an extension and expansion of Internet-based network, which expands
the communication from human and human to human and
things or things and things. In the IoT paradigm, many objects
surrounding us will be connected into networks in one form
or another. RF identification (RFID), sensor technology, and
other smart technologies will be embedded into a variety of
applications.

As a burgeoning thing, there is not a common accepted
definition on IoT. Specialists from different perspectives and
organizations describe IoT on diverse preference. Typical
definitions of IoT from different organizations are shown in
Table I.
Following technology evolutions, more and more computing
power, storage, and battery capacities become available at
relatively low cost and low size. This trend is enabling the
development of extreme small-scale electronic devices with
identification/communication/computing capabilities, which
could be embedded in other devices, systems, and facilities [1].
IoT should have the following three characteristics [6].
1) Comprehensive Perception: Using RFID, sensors, and
two-dimensional barcode to obtain the object information at anytime and anywhere, it will be a new
opportunity. Using it, information and communication
systems can be invisibly embedded in the environment around us. Sensor network will enable people
to interact with the real world remotely. Identification
technologies mentioned here include objects and location identifications. Identification and recognition of the
physical world is the foundation of implementing overall
perception.
2) Reliable Transmission: Through a variety of available radio networks, telecommunication networks, and Internet,
objects information can be available in any time. Communication technology here includes a variety of wired
and wireless transmission technologies, switching technologies, networking technologies, and gateway technologies. IoT further creates the interaction among the
physical world, the virtual world, the digital world, and
the society. Machine to machine (M2M), furthermore,
is the key implementation technology of the Network of
Things, which represents the connections and communications between M2M and Human to Machine including
Mobile to Machine.
3) Intelligent Processing: By collecting IoT data into
databases, various intelligent computing technologies including cloud computing will be able to support IoT data
applications. The network service providers can process
tens of millions or even billion pieces of messages
instantly through cloud computing. Cloud computing
technology will thus be the promoter of IoT.
Currently, in China, there are at least 9 billion interconnected devices, and it is expected to reach 24 billion devices
by 2020. According to the GSMA, this amount to $1.3 trillion

II. OPPORTUNITY, STATUS , AND CAPABILITY OF IOT
A. Opportunity of IoT

revenue opportunities for mobile network operators alone
spanning segments such as health, automotive, utilities, and
consumer electronics. Many countries consider IoT as strategic
industries and a new economic growth engine in the future.
European Union (EU) has invested more than 100 million
Euros in a series of projects through Seventh EU Framework
Programme (FP7 for R&D), and these projects will be actively
deployed in smart grid, intelligent transportation, smart cities,
etc. South Korea invested 27.8 million U.S. dollars in IoT fundamental technology development, IoT test bed advancement,
and IoT standardization, etc.
China is speeding up the development of IoT and defines
it as a new engine for economic growth. The government
released the 12th Five-Year Plan for IoT development. This
plan is an outline program for developing IoT from 2011
to 2015. The plan gave the goal and objectives of future development, and the plan proposed several approaches
reaching the goal. The plan also puts forward a list of
methods to support and promote the development of IoT
industry.
This paper is organized as follows. Section II introduces
opportunities of IoT and summarizes its status and applications. Section III introduces the policy, R&D plans, and
standardization of IoT in China. Based on the introduction
of Sections II and III, in Section IV, the major problem
hampering the development of IoT is analyzed, i.e., lack
of interoperability among diverse IoT solutions. A general
open IoT architecture developed in China is given. Section V
introduces main application fields, examples of typical public
applications, and examples of industry applications of IoT
in China. Section VI analyzes further challenges on IoT,
including technical challenge and standard challenge, and
introduces the prospect of IoT. Section VII is the conclusion
of this paper.

The IoT will create a huge network of billions or trillions of
“Things” communicating each other. The IoT is not subversive
revolution over the existing technologies, it is comprehensive
utilizations of existing technologies, and it is the creation of
the new communication modes. The IoT blends the virtual
world and the physical world by bringing different concepts
and technical components together: pervasive networks, miniaturization of devices, mobile communication, and new ecosystem. In IoT, applications, services, middleware components,
networks, and end nodes will be structurally organized and
used in entire new ways.
IoT offers a means to look into complex processes and
relationships. The IoT implies a symbiotic interaction between the real/physical and the digital/virtual worlds: physical
entities have digital counterparts and virtual representation;
things become context aware and they can sense, communicate, interact, and exchange data, information, and knowledge.
New opportunities will meet business requirements, and new
services will be created based on real-time physical world data.
Everything from the physical or virtual world will possibly
be connected by the IoT. Connectivity between the things shall
be available to all with low cost and may not be owned by
private entities. For IoT, intelligent learning, fast deployment,
best information understanding and interpreting, against fraud
and malicious attack, and privacy protection are essential
requirements.
B. Status of IoT
The IoT can be regarded as an extension of existing interaction between people and applications through a new dimension
of “Things” for communication and integration.
The IoT development process is a complex large-scale
technological innovation process. The IoT is evolving from
the vertical application to polymeric application.
At the early stage of IoT deployment, driving of domainspecific applications is the main development strategy.
A domain-specific application might be a manufacturing control system with its own industry characteristics. The application can provide various enterprise management services being
integrated with the industry production and business processes.
Polymeric applications are cross-industry applications based
on public information service platforms. These applications
support both home users and industry users. The application
are provided and promoted by communication operators and
solution providers with large scale. For example, a vehicle
integrated with sensor networks, a global positioning system
(GPS), and radio communication technology can provide
comprehensive detection, navigation, entertainment, and other
information services. By maintaining such information through
the public service platform, consumers, original equipment
manufacturers (OEMs), maintenance providers, and vehicle
management agencies can share these information and share
services to improve the vehicle, the vehicle component design, and the fabrication process through the vehicle lifecycle
management.

C. Capability of the IoT Application
In summary, the IoT applications shall have the following
capabilities.
1) Location Sensing and Sharing of Location Info: The IoT
system can collect the location information of IoT terminals
and end nodes, and then provide services based on the collected location information. The location information includes
geographical position information got from the GPS, CellID, RFID, etc., and absolute or relative position information
between things. More typical IoT applications include at least
the following.
a) Mobile asset tracking: This application can track and
monitor the status of commodity using the positionsensing device and communication function installed on
the commodity.
b) Fleet management: The manger of the fleet can schedule
the vehicles and drivers based on the business requirements and the real-time position information collected
by the vehicles.
c) Traffic information system: This application can get
traffic information such as road traffic conditions and
congested locations by tracking the location information
of a large number of vehicles. The system thus assists
the driver to choose the most efficient route.
2) Environment Sensing: The IoT system can collect and
process all kinds of physical or chemical environmental
parameters via the locally or widely deployed terminals. Typical environmental information includes temperature, humidity,
noise, visibility, light intensity, spectrum, radiation, pollution
(CO, CO2 , etc.), images, and body indicators. Typical applications include at least the following.
a) Environment detection: IoT systems offer environmental
and ecological, such as forest and glacier, monitoring;
disaster, such as volcanoes and seismic, monitoring;
and factory monitoring. All are with automatic alarm
systems using environmental parameters collected by
large number of sensors.
b) Remote medical monitoring: IoT can analyze the recurring indicator data collected from the device placed on
patients’ body and provide the users with health trends
and health advice.
3) Remote Controlling: IoT systems can control IoT
terminals and execute functions based on application commands combined with information collected from things and
service requirements.
a) Appliance control: People can remotely control operating status of appliances through IoT system.
b) Disaster recovery: Users can remotely start disasters
treatment facilities to minimize losses caused by disasters according to the monitoring mentioned before.
4) Ad Hoc Networking: IoT system shall have rapidly selforganized networking capability and can interoperate with the
network/service layer to provide related services [7]. In the
vehicle network, in order to transfer the data, the network
between vehicles and/or road infrastructures can be rapidly
self-organized.
5) Secure Communication: IoT system can further establish
secure data transmission channel between the application
or service platform and IoT terminals based on service
requirements.
In practice, an IoT application consists of different types of
capabilities and even applications based on the service requirement. Table II shows examples of different IoT applications.
III. IOT IN CHINA
A. China Pushes Development of IoT
The IoT sensing network research started in China in 1999.
IoT was positioned as one of the strategic emerging industries
and written into the government work report in March 2010.
The state council’s decision on speeding up the incubation
and development of strategic emerging industries, which was
promulgated in November 2010, clearly stated that the IoT
research and application demonstrations will be promoted.
In 2012, the Ministry of Industry and Information of
China explained the national 12th Five-Year Plan including
IoT development (2011–2015) [8]. It was the first plan that
China government released the development of IoT in detail.
The plan clearly proposed the development goal of the IoT
during the period of 2011–2015. By 2015, there should be
significant achievements of IoT essential technologies, related
applications, and standardizations.
The plan put forward eight main tasks and clearly clarified
five key projects, including the Key Technology Innovation
Project, Standardization acceleration Project, “10 Industrial
sectors & 100 New Enterprises” Industry Development Pioneering Project, Application Demonstration Projects in Key
Sectors, and Public Service Platform Construction Project.
In order to solve the popped-up problems and consider
long-term development of IoT, the State Council issued the
“guidance on tracking and ordering for promoting the development of IoT” [9] that determined the development goals
and threads for IoT in February 2013. The 14 ministries of
IEEE INTERNET OF THINGS JOURNAL, VOL. 1, NO. 4, AUGUST 2014

China established a joint meeting for IoT development in
September 2013, and organized Expert Advisory Committee
on the IoT development. The 10 Special Development Action
Plans for IoT [10], including: 1) top-layer design; 2) standards
development; 3) technology development; 4) application promotion; 5) industry support; 6) business models; 7) security;
8) government support; 9) laws and regulations guarantee;
and 10) personnel training, was issued by the joint meeting.
As a part of the Action Plans, the strategic alliance for
industrial technology innovations of IoT was established in
October 2013 [11]. Fig. 1 shows the summary of the Special
Development Packet Plans.
B. R&D Plans
In China, the central government established the special
funds for demonstration projects and research projects to
support the development of IoT. In 2011, for supporting the
development of IoT in China, around RMB 500 million special
IoT fund was invested into IoT-related fields, 2/3 of the funds
were put into R&D and applications; this fund has supported
381 related companies since 2011. China government has
supported 22 National major IoT application demonstration
projects since 2011, and an announcement was issued by China
National Development and Reform Commission in October
2013. It was about organizing and carrying out the national IoT
pilot major application demonstration projects during 2014–
2016 in special regions.
In Research and Development area, the China Ministry
of Industry and Information Technology had set up a number of key technical research projects on architectures and
applications such as intelligent transport system (ITS) and
e-health under the packet of “a new generation of mobile
broadband project.” China Ministry of Science and Technology
also set up a series of fundamental researches for IoT on

architecture, fundamental theory and design, etc., under the
973 project framework (National Key Fundamental Research
and Development Plan).
The national level IoT R&D is distributed as: the enterprises such as operators and vendors provide the operation
and system development of IoT. Universities and research
institutions focus on the key technology research, and the
standard organizations are responsible for the standardization
of IoT. At present, IoT-related industry has basically formed,
and mainly distributed over the Bohai bay area, the Yangtze
River Delta zoon, the Pearl River Delta zoon, and the central
and western regions. Fig. 2 shows the summary of national
R&D plans in IoT.
C. Standardization
The IoT standard system contains the architecture standards,
the application requirements standards, the communication
protocol standards, the identification standards, the security
standards, the application standards, the data standards, the
information processing standards, and the public service platform standards.
The proposed IoT standard set is relatively complicated. In
China, the standardization efforts started at 2010. The main
standard organizations for IoT in China are China Communications Standards Association (CCSA), China Standardization
Working Group on Sensor Networks (WGSN), electric tag
standards technical committee, etc. These standard organizations are leading the standardization process of China IoT.
As part of the Special IoT Action Plans, actions for the standardization of IoT include to build a standard system, to develop common standards, key technical standards, and urgent
industry standards, to actively participate in the international
standardization processes, to conduct standard validations and
services, to improve the organizational structure.
In the course of standardization, many research institutions
and enterprises in China have also been participating in
international standardization work in M2M of International Organization for Standardization/International Electrotechnical
Commission (ISO/IEC), ITU Telecommunication Standardization Sector (ITU-T), 3rd Generation Partnership Project
(3GPP). China is one of the leading countries in ITU-T and
ISO Wireless Sensor Network (WSN) Working Group. CCSA
is one of the sponsoring organizations of the One M2M, and
many enterprises are deeply participating in the MTC-related
standard development in 3GPP.
IV. OPEN AND GENERAL IOT ARCHITECTURE
A. Motivation and General Description
As seen in the previous introduction to the current IoT, most
IoT applications in China were domain-specific or applicationspecific solutions. The architectures of these IoT systems
are fragmented and cannot correlate and integrate the data
from different silos; these isolated IoT solutions use private
protocols and cause much problems in information sharing,
technology multiplexing, network managements, and upgrading. All these problems are hindering the development of IoT.
In order to reduce the total IoT cost and share information,
we need to integrate multiple functions and resources into a
larger system. IoT thus needs to be designed with an open and
generic IoT architecture with open interfaces and resources,
considering different business scenarios, application-based requirements, and current technologies [12]. We have thus seen
the motivation to formulate a standard for IoT integration in
order to reduce the total cost of money and time from devices,
developments, and deployments.
An open and generic IoT architecture is an integrated
solution with interoperability [13], [14]. It will have the
following characteristics.
1) Standard Interface and Protocol: By comparing various
private IoT systems, a generic IoT infrastructure has the same
hardware and software interfaces, and protocols.
2) Public and Operating: A general IoT architecture is
deployed to take over public IoT applications with openoperating capability. A public IoT system can thus integrate
multiple IoT applications into one architecture.
3) Open, Scalable, and Flexible: An open IoT architecture
with open resources, open standards, and open interfaces can
easily extend its functionality and the scale of performance.
It can thus adapt to different requirements including technical
developments flexibly.
B. Open and General IoT Architecture
China Communications Standards Association (CCSA)
proposed a reference model for the IoT, which consists of
sensing layer, network and business layers, and application
layer. Complying with this reference model, Fig. 3 shows its
open and general architecture, which is layered, open, and
flexible. The architecture includes three functional platforms
as follows.
1) Sensing and Gateway Platform: This platform connects
sensors, controllers, RFID readers, and location sensing device
(e.g., GPS) to IoT network layer. Modularization of hardware,
data format, and software interface is proposed for IoT terminal, IoT Gateway, and tip node. IoT terminal, IoT gateway, and
tip node can include flexible modules combined with control

module, common interface module, and communication module. Common interface module collects physical interfaces of
various sensors into a common interface. Common control
module can connect sensors, controllers, GPS, and RFID
readers with a common connection protocol. The software and
application parameters of an IoT terminal and IoT gateway
should be able to self-configure and self-adapt. Modularization, common interface, intelligent operation, self-adaption,
and self-configuration are important characteristics of this
platform.
2) Resource and Administration Platform: Network and
service layer includes backbone networks and resource administration platforms. The backbone network includes 3G,
4G, internet, optical fiber network, Ethernet network, satellite
networks, and private network. The resource and administration platform provides common capabilities which can be
used by different IoT applications, such as data processing,
data storage, security management, and application supporting. These capabilities may also be invoked by specific IoT
application support capabilities, e.g., to build other specific
IoT application support capabilities. This platform also provides relevant control functions of network connectivity, such
as access and transport resource control functions, mobility
management, or authentication, authorization, and accounting
for IoT terminals, services, applications, users, and developers.
3) Open Application Platform: Modularization design in
this application platform provides common function and open
application programming interface (API). An IoT application provider can develop its application using these APIs.
Meanwhile, this platform supports application managements.
Various applications can be published to the application platform and users can get application information and subscribe
applications through this platform. Convenient and easy deployment, distribution, and flexible application environment
are the characteristics of this platform.
V. T YPICAL A PPLICATIONS AND D EPLOYMENT IN C HINA
A. Main Application Fields
In China, IoT applications will be developed in nine fields,
including: 1) domain industry applications; 2) smart agriculture; 3) smart logistics; 4) intelligent transportation; 5) smart
grid; 6) smart environmental protection; 7) smart safety; 8)
smart medical care; and 9) smart home, as shown in Table III.

B. Deployment of Typical Applications

Three major operators: 1) China mobile; 2) China Telecom;
and 3) China Unicom had announced their IoT development
plans. The IoT has been an important part in the strategies of
Chinese telecom operators.
China mobile plans to implement a centralized platform
[China mobile IoT operation supporting platform (CMITS)],
and the branch for IoT was established in Chongqing in
2012.
China Telecom plans to further enrich IoT services based
on key industry requirements, to build an open platform for
cooperation, and to promote IoT business model innovation.
The branch for IoT was established in Wuxi in 2014.
China Unicom plans to build a management and control platform and an operation supporting system for IoT.
They also planned to offer specific hardware communication
modules.
The government planned to provide special phone numbers
(1064xxxxxxxxx with billions of identities) dedicated for
M2M. Table IV shows IoT development plans and actions
from three major operators.

1) Smart City: Smart city is a new development model of
a city using new technologies, such as IoT, cloud computing,
and big data analytics, to boost the information sharing and
coordination within a city system. IoT is important means
and tools of building smart city, and it is infrastructurecarrying smart city construction. Smart city construction depends on a lot of IoT applications for different industries
[15], [16].
Smart city development plans are divided into three stages:
1) the stage for initial infrastructure construction; 2) the stage
for data-processing facility construction; and 3) the stage for
end-phase service platform construction. A large number of
smart city projects provide huge opportunities for telecom
OEM, systems integration enterprises, data aggregation and
analysis/service enterprises, and telecom operators. In China,
some cities are focusing on improving infrastructure for smart
cities programs, and on developing regional smart e-commerce
and logistics, while other cities are stepping up efforts to
improve the management of local utilities. According to estimation, the smart city market will have a total value of more
than RMB 2 Trillion during the 12th Five-Year Plan period
(2011–2015).
2) Intelligent Transportation: China needs a solution to
solve the problems of increased amount of traffic congestion
in urban areas using new technologies such as IoT.
The Ministry of Transport in China has announced a plan
that the development of ITS will be installed and in-use across
the country by 2020. In the next 10 years, the government will
invest about RMB 4 Trillion in intelligent transportation.

Intelligent Transportation System in China is developing
and keeping a high growth rate. The growing speed of China’s
urban intelligent transportation control system market is high,
including electronic police, intelligent traffic signal control,
traffic video monitoring, intelligent Taxi service management,
urban public transport information technology, and ETC.
Until the completion of the 12th Five-Year Plan, 60% of
national highways in China will have ETC. The installation
of IoT infrastructures in ITS of smart cities will be supporting
fundamental technologies.
Connected vehicles and vehicular networks have been identified as a key technology for enhancing road safety and
transport efficiency [17], [18]. In recent years, vehicular
network infrastructure integration technology attracts a great
amount of attentions; it also brings inestimable economic
value, and will play an important role in the next generation of
intelligent transportation systems and communication network
development. Separately, built-in connectivity in vehicles is
predicted to mandatory function for consumers in 2020, driven
partly by the increasing global demand for availability of
charging supply for electric vehicles. Government, University,
automakers, and telecommunication vendors form alliances to
develop this industry, such as China ITS Industry Alliance
and Telematics Industry Application Alliance, to develop the
connected vehicles industry. The related installation of IoT
infrastructures of smart cities will be fundamental supporting
technologies as well.

C. Deployment of Typical Industry Applications
1) Intelligent Coal Mine: In 2011, The State Administration of work safety and State Coal Mine Safety Supervision
Bureau in China regulated that all coal mines in china must
complete the construction of “Mine Six-Hedge Safety Systems” until the end of 2013. The Six-Hedge underground systems include: 1) general monitoring and controlling system; 2)
personnel positioning system; 3) emergent shelter systems; 4)
oxygen provision monitoring system; 5) water supply and
drainage monitoring system; and 6) mine cable and wireless
communication system.
All the six systems mentioned above are based on pervasive
sensing, sensing data collection, real-time use of data, and
deep data post analyses, which is actually based on IoT. The
IoT can be widely used in the “six system.” For example,
it uses sensors to monitor miner position and leak of hazard
gases. Fig. 4 shows a typical underground IoT system.
Built on underground industrial ring network and industrial
field bus, the system is actually a comprehensive underground
IoT platform, which transmits data of mining devices under
industrial Ethernet protocols to dispatch center so as to monitor various devices and sensors inside the dispatch center.
The overall design of the system consists of sensing and
equipment layer, network and access layer, as well application
and information management layer. The system collects realtime parameters of electromechanical devices and data of
safety systems, which improves the system efficiency and
safety. Finally, it highly guarantees the correct dispatching and
decision-making of coal mines.

2) Intelligent Oil Field: Intelligent oil field (IOF) consists
of a distributed system, holding frequently captured data. The
data are evaluated and acted upon real-time information. IOF
brings lower operational costs, lower capital investment, and
increased yield of oil and gas.
Fig. 5 shows the IOF system based on the architecture
mentioned in Section IV. It will collect parameters through
its entire production process, such as sensing and monitoring
parameters from oil well and gas well, metering house, oil station and gas station, and oil and gas pipe network. Sensed data
will be collected and transmitted for further analysis between
the service platform and gateway/terminal with a common
connection protocol. Real-time production data and equipment
condition information support the centralized management and
control at production command center in production control
center. IOF can increase the integrity, accuracy, timeliness,
and standardization of the production decision-making for oil
and gas fields.

IEEE INTERNET OF THINGS JOURNAL, VOL. 1, NO. 4, AUGUST 2014

The goal and objectives of the IOF system are to increase
the accuracy and timeliness of product process, decisionmaking, improvement of the system management level, reducing the operation costs, and minimizing risk. As a subsystem
in IOF, IoT system offers automatic production data collection,
remote monitoring, and supporting for production prewarning
by establishing a standardized general service platform, which
covers all oil and gas wells and fields, metering house, collect
and conveyance station, combination station, treatment plant.
The platform also supports the process of production process
management."
"Edge Computing: Vision and Challenges
Weisong Shi, Fellow, IEEE, Jie Cao, Student Member, IEEE, Quan Zhang, Student Member, IEEE,
Youhuizi Li, and Lanyu Xu

Abstract—The proliferation of Internet of Things (IoT) and
the success of rich cloud services have pushed the horizon of
a new computing paradigm, edge computing, which calls for
processing the data at the edge of the network. Edge computing has the potential to address the concerns of response time
requirement, battery life constraint, bandwidth cost saving, as
well as data safety and privacy. In this paper, we introduce the
definition of edge computing, followed by several case studies,
ranging from cloud offloading to smart home and city, as well
as collaborative edge to materialize the concept of edge computing. Finally, we present several challenges and opportunities
in the field of edge computing, and hope this paper will gain
attention from the community and inspire more research in this
direction.
Index Terms—Edge computing, Internet of Things (IoT), smart
home and city.

I. INTRODUCTION
LOUD computing has tremendously changed the way we
live, work, and study since its inception around 2005 [1].
For example, software as a service (SaaS) instances, such as
Google Apps, Twitter, Facebook, and Flickr, have been widely
used in our daily life. Moreover, scalable infrastructures as
well as processing engines developed to support cloud service
are also significantly influencing the way of running business,
for instance, Google File System [2], MapReduce [3], Apache
Hadoop [4], Apache Spark [5], and so on.
Internet of Things (IoT) was first introduced to the community in 1999 for supply chain management [6], and then
the concept of “making a computer sense information without the aid of human intervention” was widely adapted to
other fields such as healthcare, home, environment, and transports [7], [8]. Now with IoT, we will arrive in the post-cloud
era, where there will be a large quality of data generated
by things that are immersed in our daily life, and a lot of
applications will also be deployed at the edge to consume
these data. By 2019, data produced by people, machines, and
things will reach 500 zettabytes, as estimated by Cisco Global
Cloud Index, however, the global data center IP traffic will
only reach 10.4 zettabytes by that time [9]. By 2019, 45% of
IoT-created data will be stored, processed, analyzed, and acted
upon close to, or at the edge of, the network [10]. There will

be 50 billion things connected to the Internet by 2020, as predicted by Cisco Internet Business Solutions Group [11]. Some
IoT applications might require very short response time, some
might involve private data, and some might produce a large
quantity of data which could be a heavy load for networks.
Cloud computing is not efficient enough to support these
applications.
With the push from cloud services and pull from IoT,
we envision that the edge of the network is changing from
data consumer to data producer as well as data consumer.
In this paper, we attempt to contribute the concept of edge
computing. We start from the analysis of why we need
edge computing, then we give our definition and vision of
edge computing. Several case studies like cloud offloading,
smart home and city as well as collaborative edge are introduced to further explain edge computing in a detailed manner,
followed by some challenges and opportunities in programmability, naming, data abstraction, service management, privacy
and security, as well as optimization metrics that are worth
future research and study.
The remaining parts of this paper are organized as follows.
Section II discusses the need for edge computing as well as
gives the definition of edge computing. In Section III, we
show some edge computing case studies. Section IV presents
the possible challenges and opportunities. Finally, this paper
concludes in Section V.
II. WHAT IS EDGE COMPUTING
Data is increasingly produced at the edge of the network,
therefore, it would be more efficient to also process the data at
the edge of the network. Previous work such as micro datacenter [12], [13], cloudlet [14], and fog computing [15] has been
introduced to the community because cloud computing is not
always efficient for data processing when the data is produced
at the edge of the network. In this section, we list some reasons
why edge computing is more efficient than cloud computing
for some computing services, then we give our definition and
understanding of edge computing.
A. Why Do We Need Edge Computing
1) Push From Cloud Services: Putting all the computing
tasks on the cloud has been proved to be an efficient way
for data processing since the computing power on the cloud
outclasses the capability of the things at the edge. However,
compared to the fast developing data processing speed, the
bandwidth of the network has come to a standstill. With the
growing quantity of data generated at the edge, speed of data

Cloud computing paradigm.

transportation is becoming the bottleneck for the cloud-based
computing paradigm. For example, about 5 Gigabyte data will
be generated by a Boeing 787 every second [16], but the
bandwidth between the airplane and either satellite or base
station on the ground is not large enough for data transmission. Consider an autonomous vehicle as another example. One
Gigabyte data will be generated by the car every second and it
requires real-time processing for the vehicle to make correct
decisions [17]. If all the data needs to be sent to the cloud
for processing, the response time would be too long. Not to
mention that current network bandwidth and reliability would
be challenged for its capability of supporting a large number
of vehicles in one area. In this case, the data needs to be processed at the edge for shorter response time, more efficient
processing and smaller network pressure.
2) Pull From IoT: Almost all kinds of electrical devices will
become part of IoT, and they will play the role of data producers as well as consumers, such as air quality sensors, LED
bars, streetlights and even an Internet-connected microwave
oven. It is safe to infer that the number of things at the edge
of the network will develop to more than billions in a few
years. Thus, raw data produced by them will be enormous,
making conventional cloud computing not efficient enough to
handle all these data. This means most of the data produced
by IoT will never be transmitted to the cloud, instead it will
be consumed at the edge of the network.
Fig. 1 shows the conventional cloud computing structure.
Data producers generate raw data and transfer it to cloud, and
data consumers send request for consuming data to cloud, as
noted by the blue solid line. The red dotted line indicates
the request for consuming data being sent from data consumers to cloud, and the result from cloud is represented by
the green dotted line. However, this structure is not sufficient
for IoT. First, data quantity at the edge is too large, which will
lead to huge unnecessary bandwidth and computing resource
usage. Second, the privacy protection requirement will pose
an obstacle for cloud computing in IoT. Lastly, most of the
end nodes in IoT are energy constrained things, and the wireless communication module is usually very energy hungry, so
offloading some computing tasks to the edge could be more
energy efficient.
3) Change From Data Consumer to Producer: In the cloud
computing paradigm, the end devices at the edge usually play
as data consumer, for example, watching a YouTube video on
your smart phone. However, people are also producing data
nowadays from their mobile devices. The change from data
consumer to data producer/consumer requires more function
placement at the edge. For example, it is very normal that
people today take photos or do video recording then share
the data through a cloud service such as YouTube, Facebook,
Twitter, or Instagram. Moreover, every single minute, YouTube

Edge computing paradigm.

users upload 72 h of new video content; Facebook users share
nearly 2.5 million pieces of content; Twitter users tweet nearly
300 000 times; Instagram users post nearly 220 000 new photos [18]. However, the image or video clip could be fairly
large and it would occupy a lot of bandwidth for uploading.
In this case, the video clip should be demised and adjusted
to suitable resolution at the edge before uploading to cloud.
Another example would be wearable health devices. Since the
physical data collected by the things at the edge of the network is usually private, processing the data at the edge could
protect user privacy better than uploading raw data to cloud.

B. What Is Edge Computing
Edge computing refers to the enabling technologies allowing computation to be performed at the edge of the network,
on downstream data on behalf of cloud services and upstream
data on behalf of IoT services. Here we define “edge” as any
computing and network resources along the path between data
sources and cloud data centers. For example, a smart phone is
the edge between body things and cloud, a gateway in a smart
home is the edge between home things and cloud, a micro data
center and a cloudlet [14] is the edge between a mobile device
and cloud. The rationale of edge computing is that computing
should happen at the proximity of data sources. From our point
of view, edge computing is interchangeable with fog computing [19], but edge computing focus more toward the things
side, while fog computing focus more on the infrastructure
side. We envision that edge computing could have as big an
impact on our society as has the cloud computing.
Fig. 2 illustrates the two-way computing streams in edge
computing. In the edge computing paradigm, the things not
only are data consumers, but also play as data producers. At
the edge, the things can not only request service and content
from the cloud but also perform the computing tasks from
the cloud. Edge can perform computing offloading, data storage, caching and processing, as well as distribute request and
delivery service from cloud to user. With those jobs in the
network, the edge itself needs to be well designed to meet the
requirement efficiently in service such as reliability, security,
and privacy protection.
C. Edge Computing Benefits
In edge computing we want to put the computing at the
proximity of data sources. This have several benefits compared to traditional cloud-based computing paradigm. Here we
use several early results from the community to demonstrate
the potential benefits. Researchers built a proof-of-concept
platform to run face recognition application in [20], and the
response time is reduced from 900 to 169 ms by moving computation from cloud to the edge. Ha et al. [21] used cloudlets
to offload computing tasks for wearable cognitive assistance,
and the result shows that the improvement of response time is
between 80 and 200ms. Moreover, the energy consumption
could also be reduced by 30%–40% by cloudlet offloading. clonecloud in [22] combine partitioning, migration with
merging, and on-demand instantiation of partitioning between
mobile and the cloud, and their prototype could reduce 20×
running time and energy for tested applications.
III. CASE STUDY
In this section, we give several case studies where edge
computing could shine to further illustrate our vision of edge
computing.
A. Cloud Offloading
In the cloud computing paradigm, most of the computations happen in the cloud, which means data and requests
are processed in the centralized cloud. However, such a
computing paradigm may suffer longer latency (e.g., long
tail latency), which weakens the user experience. Numbers
of researches have addressed the cloud offloading in terms
of energy-performance tradeoff in a mobile-cloud environment [22]–[26]. In edge computing, the edge has certain
computation resources, and this provides a chance to offload
part of the workload from cloud.
In the traditional content delivery network, only the data is
cached at the edge servers. This is based on the fact that the
content provider provides the data on the Internet, which is
true for the past decades. In the IoT, the data is produced and
consumed at the edge. Thus, in the edge computing paradigm,
not only data but also operations applied on the data should
be cached at the edge.
One potential application that could benefit from edge
computing is online shopping services. A customer may
manipulate the shopping cart frequently. By default, all these
changes on his/her shopping cart will be done in the cloud,
and then the new shopping cart view is updated on the customer’s device. This process may take a long time depending
on network speed and the load level of servers. It could be
even longer for mobile devices due to the relatively low bandwidth of a mobile network. As shopping with mobile devices
is becoming more and more popular, it is important to improve
the user experience, especially latency related. In such a scenario, if the shopping cart updating is offloaded from cloud
servers to edge nodes, the latency will be dramatically reduced.
As we mentioned, the users’ shopping cart data and related
operations (e.g., add an item, update an item, delete an item)
both can be cached at the edge node. The new shopping cart
view can be generated immediately upon the user request
reaching the edge node. Of course, the data at the edge node
should be synchronized with the cloud, however, this can be
done in the background.
Another issue involves the collaboration of multiple edges
when a user moves from one edge node to another. One simple
solution is to cache the data to all edges the user may reach.
Then the synchronization issue between edge nodes rises up.
All these issues could become challenges for future investigation. At the bottom line, we can improve the interactive
services quality by reducing the latency. Similar applications
also include the following.
1) Navigation applications can move the navigating or
searching services to the edge for a local area, in which
case only a few map blocks are involved.
2) Content filtering/aggregating could be done at the edge
nodes to reduce the data volume to be transferred.
3) Real-time applications such as vision-aid entertainment
games, augmented reality, and connected health, could
make fast responses by using edge nodes.
Thus, by leveraging edge computing, the latency and consequently the user experience for time-sensitive application
could be improved significantly.
B. Video Analytics
The widespread of mobilephones and network cameras
make video analytics an emerging technology. Cloud computing is no longer suitable for applications that requires video
analytics due to the long data transmission latency and privacy
concerns. Here we give an example of finding a lost child
in the city. Nowadays, different kinds of cameras are widely
deployed in the urban area and in each vehicle. When a child
is missing, it is very possible that this child can be captured by
a camera. However, the data from the camera will usually not
be uploaded to the cloud because of privacy issues or traffic
cost, which makes it extremely difficult to leverage the wide
area camera data. Even if the data is accessible on the cloud,
uploading and searching a huge quantity of data could take a
long time, which is not tolerable for searching a missing child.
With the edge computing paradigm, the request of searching
a child can be generated from the cloud and pushed to all the
things in a target area. Each thing, for example, a smart phone,
can perform the request and search its local camera data and
only report the result back to the cloud. In this paradigm, it is
possible to leverage the data and computing power on every
thing and get the result much faster compared with solitary
cloud computing.
C. Smart Home
IoT would benefit the home environment a lot. Some products have been developed and are available on the market such
as smart light, smart TV, and robot vacuum. However, just
adding a Wi-Fi module to the current electrical device and
connecting it to the cloud is not enough for a smart home.
In a smart home environment, besides the connected device,
cheap wireless sensors and controllers should be deployed to
room, pipe, and even floor and wall. These things would report
an impressive amount of data and for the consideration of
data transportation pressure and privacy protection, this data
should be mostly consumed in the home. This feature makes
the cloud computing paradigm unsuitable for a smart home.
Nevertheless, edge computing is considered perfect for building a smart home: with an edge gateway running a specialized
edge operating system (edgeOS) in the home, the things can
be connected and managed easily in the home, the data can
be processed locally to release the burdens for Internet bandwidth, and the service can also be deployed on the edgeOS
for better management and delivery. More opportunities and
potential challenges are discussed in Section IV.
Fig. 3 shows the structure of a variant of edgeOS in the
smart home environment. EdgeOS needs to collect data from
mobile devices and all kinds of things through multiple communication methods such as Wi-Fi, BlueTooth, ZigBee, or
a cellular network. Data from different sources needs to be
fused and massaged in the data abstraction layer. Detailed
description of this process will be discussed in Section IV-C.
On top of the data abstraction layer is the service management layer. Requirements including differentiation, extensibility, isolation, and reliability will be supported in this
layer. In Section IV-D, this issue will be further addressed.
The naming mechanism is required for all layers with different requirements. Thus, we leave the naming module in
a cross-layer fashion. Challenges in naming are discussed
in Section IV-B.
D. Smart City
The edge computing paradigm can be flexibly expanded
from a single home to community, or even city scale. Edge
computing claims that computing should happen as close as
possible to the data source. With this design, a request could
be generated from the top of the computing paradigm and
be actually processed at the edge. Edge computing could be
an ideal platform for smart city considering the following
characteristics.
1) Large Data Quantity: A city populated by 1 million people will produce 180 PB data per day by 2019 [9], contributed
by public safety, health, utility, and transports, etc. Building
centralized cloud data centers to handle all of the data is unrealistic because the traffic workload would be too heavy. In
this case, edge computing could be an efficient solution by
processing the data at the edge of the network.
2) Low Latency: For applications that require predictable
and low latency such as health emergency or public safety,

Collaborative edge example: connected health.

edge computing is also an appropriate paradigm since it could
save the data transmission time as well as simplify the network
structure. Decision and diagnosis could be made as well as distributed from the edge of the network, which is more efficient
compared with collecting information and making decision at
central cloud.
3) Location Awareness: For geographic-based applications
such as transportation and utility management, edge computing
exceed cloud computing due to the location awareness. In edge
computing, data could be collected and processed based on
geographic location without being transported to cloud.
E. Collaborative Edge
Cloud, arguably, has become the de facto computing platform for the big data processing by academia and industry. A
key promise behind cloud computing is that the data should
be already held or is being transmitted to the cloud and will
eventually be processed in the cloud. In many cases, however,
the data owned by stakeholders is rarely shared to each other
due to privacy concerns and the formidable cost of data transportation. Thus, the chance of collaboration among multiple
stake-holders is limited. Edge, as a physical small data center
that connects cloud and end user with data processing capability, can also be part of the logical concept. collaborative edge,
which connects the edges of multiple stakeholders that are
geographically distributed despite their physical location and
network structure is proposed [15]. Those ad hoc-like connected edges provide the opportunity for stakeholders to share
and cooperate data.
One of the promising applications in the near future is
connected health, as shown in Fig. 4. The demand of geographically distributed data processing applications, i.e., healthcare,
requires data sharing and collaboration among enterprises in
multiple domains. To attack this challenge, collaborative edge
can fuse geographically distributed data by creating virtual
shared data views. The virtual shared data is exposed to end
users via a predefined service interface. An application will
leverage this public interface to compose complex services
for end users. These public services are provided by participants of collaborative edge, and the computation only occurs
in the participant’s data facility such that the data privacy and
integrity can be ensured.
To show the potential benefits of collaborative edge, we
use connected healthcare as a case study. We use a flu outbreak as the beginning of our case study. The patients flow
to hospitals, and the electronic medical record (EMR) of the
patients will be updated. The hospital summarizes and shares
the information for this flu outbreak, such as the average cost,
the symptoms, and the population, etc. A patient theoretically
will follow the prescription to get the pills from a pharmacy.
One possibility is that a patient did not follow the therapy.
Then the hospital has to take the responsibility for rehospitalization since it cannot get the proof that the patient did
not take the pills. Now, via collaborative edge, the pharmacy
can provide the purchasing record of a patient to the hospital,
which significantly facilitates healthcare accountability.
At the same time, the pharmacies retrieve the population
of the flu outbreak using the collaborative edge services provided by hospitals. An apparent benefit is that the pharmacies
have enough inventory to obtain much more profits. Behind
the drug purchasing, the pharmacy can leverage data provided
by pharmaceutical companies and retrieve the locations, prices
and inventories of all drug warehouses. It also sends a transport price query request to the logistics companies. Then the
pharmacy can make an order plan by solving the total cost
optimization problem according to retrieved information. The
pharmaceutical companies also receive a bunch of flu drug
orders from pharmacies. At this point, a pharmaceutical company can reschedule the production plan and rebalance the
inventories of the warehouses. Meanwhile, the centers for disease control and prevention, as our government representative
in our case, is monitoring the flu population increasing at wide
range areas, can consequently raise a flu alert to the people
in the involved areas. Besides, further actions can be taken to
prevent the spread of flu outbreak.
After the flu outbreak, the insurance companies have to pay
the bill for the patients based on the policy. The insurance
companies can analyze the proportion of people who has the
flu during the outbreak. This proportion and the cost for flu
treatment are significant factors to adjust the policy price for
the next year. Furthermore, the insurance companies can also
provide a personalized healthcare policy based on their EMR
if the patient would like to share it.
Through this simple case, most of the participants can benefit from collaborative edge in terms of reducing operational
cost and improving profitability. However, some of them, like
hospitals in our case, could be a pure contributor to the healthcare community since they are the major information collector
in this community.
IV. C HALLENGES AND O PPORTUNITIES
We have described five potential applications of edge computing in the last section. To realize the vision of edge
computing, we argue that the systems and network community need to work together. In this section, we will further
summarize these challenges in detail and bring forward some
potential solutions and opportunities worth further research,
including programmability, naming, data abstraction, service
management, privacy and security and optimization metrics.
A. Programmability
In cloud computing, users program their code and deploy
them on the cloud. The cloud provider is in charge to decide
where the computing is conducted in a cloud. Users have zero
or partial knowledge of how the application runs. This is one
of the benefits of cloud computing that the infrastructure is
transparent to the user. Usually, the program is written in one
programing language and compiled for a certain target platform, since the program only runs in the cloud. However, in
the edge computing, computation is offloaded from the cloud,
and the edge nodes are most likely heterogeneous platforms. In
this case, the runtime of these nodes differ from each other, and
the programmer faces huge difficulties to write an application
that may be deployed in the edge computing paradigm.
To address the programmability of edge computing, we
propose the concept of computing stream that is defined as
a serial of functions/computing applied on the data along
the data propagation path. The functions/computing could
be entire or partial functionalities of an application, and
the computing can occur anywhere on the path as long as
the application defines where the computing should be conducted. The computing stream is software defined computing
flow such that data can be processed in distributed and efficient fashion on data generating devices, edge nodes, and
the cloud environment. As defined in edge computing, a
lot of computing can be done at the edge instead of the
centric cloud. In this case, the computing stream can help
the user to determine what functions/computing should be
done and how the data is propagated after the computing
happened at the edge. The function/computing distribution
metric could be latency-driven, energy cost, TCO, and hardware/software specified limitations. The detailed cost model is
discussed in Section IV-F. By deploying a computing stream,
we expect that data is computed as close as possible to the
data source, and the data transmission cost can be reduced.
In a computing stream, the function can be reallocated, and
the data and state along with the function should also be
reallocated. Moreover, the collaboration issues (e.g., synchronization, data/state migration, etc.) have to be addressed across
multiple layers in the edge computing paradigm.
B. Naming
In edge computing, one important assumption is that the
number of things is tremendously large. At the top of the
edge nodes, there are a lot of applications running, and each
application has its own structure about how the service is provided. Similar to all computer systems, the naming scheme in
edge computing is very important for programing, addressing,
things identification, and data communication. However, an
efficient naming mechanism for the edge computing paradigm
has not been built and standardized yet. Edge practitioners
usually needs to learn various communication and network
protocols in order to communicate with the heterogeneous
things in their system. The naming scheme for edge computing
needs to handle the mobility of things, highly dynamic network
topology, privacy and security protection, as well as the scalability targeting the tremendously large amount of unreliable
things.
Traditional naming mechanisms such as DNS and uniform
resource identifier satisfy most of the current networks very

Data abstraction issue for edge computing.

Naming mechanism in edgeOS.

well. However, they are not flexible enough to serve the
dynamic edge network since sometimes most of the things
at edge could be highly mobile and resource constrained.
Moreover, for some resource constrained things at the edge
of the network, IP based naming scheme could be too heavy
to support considering its complexity and overhead.
New naming mechanisms such as named data networking (NDN) [27] and MobilityFirst [28] could also be applied
to edge computing. NDN provide a hierarchically structured
name for content/data centric network, and it is human friendly
for service management and provides good scalability for
edge. However, it would need extra proxy in order to fit into
other communication protocols such as BlueTooth or ZigBee,
and so on. Another issue associated with NDN is security,
since it is very hard to isolate things hardware information
with service providers. MobileFirst can separate name from
network address in order to provide better mobility support,
and it would be very efficient if applied to edge services where
things are of highly mobility. Neverless, a global unique identification (GUID) needs to be used for naming is MobileFirst,
and this is not required in related fixed information aggregation
service at the edge of the network such as home environment.
Another disadvantage of MobileFirst for edge is the difficulty
in service management since GUID is not human friendly.
For a relative small and fixed edge such as home environment, let the edgeOS assign network address to each thing
could be a solution. With in one system, each thing could have
a unique human friendly name which describes the following
information: location (where), role (who), and data description (what), for example, “kitchen.oven2.temperature3.” Then
the edgeOS will assign identifier and network address to this
thing, as shown in Fig. 5. The human friendly name is unique
for each thing and it will be used for service management,
things diagnosis, and component replacement. For user and
service provider, this naming mechanism makes management
very easy. For example, the user will receive a message from
edgeOS like “Bulb 3 (what) of the ceiling light (who) in living
room (where) failed,” and then the user can directly replace
the failed bulb without searching for an error code or reconfigure the network address for the new bulb. Moreover, this
naming mechanism provides better programmability to service
providers and in the meanwhile, it blocks service providers
from getting hardware information, which will protect data privacy and security better. Unique identifier and network address
could be mapped from human friendly name. Identifier will be

used for things management in edgeOS. Network address such
as IP address or MAC address will be used to support various
communication protocols such as BlueTooth, ZigBee or WiFi,
and so on. When targeting highly dynamic environment such
as city level system, we think it is still an open problem and
worth further investigation by the community.
C. Data Abstraction
Various applications can run on the edgeOS consuming
data or providing service by communicating through the air
position indicators from the service management layer. Data
abstraction has been well discussed and researched in the wireless sensor network and cloud computing paradigm. However,
in edge computing, this issue becomes more challenging. With
IoT, there would be a huge number of data generators in the
network, and here we take a smart home environment as an
example. In a smart home, almost all of the things will report
data to the edgeOS, not to mention the large number of things
deployed all around the home. However, most of the things at
the edge of the network, only periodically report sensed data
to the gateway. For example, the thermometer could report the
temperature every minute, but this data will most likely only
be consumed by the real user several times a day. Another
example could be a security camera in the home which might
keep recording and sending the video to the gateway, but the
data will just be stored in the database for a certain time with
nobody actually consuming it, and then be flushed by the latest
video.
Based on this observation, we envision that human involvement in edge computing should be minimized and the edge
node should consume/process all the data and interact with
users in a proactive fashion. In this case, data should be preprocessed at the gateway level, such as noise/low-quality removal,
event detection, and privacy protection, and so on. Processed
data will be sent to the upper layer for future service providing.
There will be several challenges in this process.
First, data reported from different things comes with various formats, as shown in Fig. 6. For the concern of
privacy and security, applications running on the gateway
should be blinded from raw data. Moreover, they should
extract the knowledge they are interested in from an integrated data table. We can easily define the table with
id, time, name, data (e.g.,{0000, 12:34:56PM 01/01/2016,
kitchen.oven2.temperature3, 78}) such that any edge thing’s
data can be fitted in. However, the details of sensed data have
been hidden, which may affect the usability of data.

Second, it is sometimes difficult to decide the degree of
data abstraction. If too much raw data is filtered out, some
applications or services could not learn enough knowledge.
However, if we want to keep a large quantity of raw data, there
would be a challenge for data storage. Lastly, data reported
by things at edge could be not reliable sometime, due to
the low precision sensor, hazard environment, and unreliable
wireless connection. In this case, how to abstract useful information from unreliable data source is still a challenge for IoT
application and system developers.
One more issue with data abstraction is the applicable operations on the things. Collecting data is to serve the application
and the application should be allowed to control (e.g., read
from and write to) the things in order to complete certain services the user desires. Combining the data representation and
operations, the data abstraction layer will serve as an public
interface for all things connected to edgeOS. Furthermore, due
the heterogeneity of the things, both data representation and
allowed operations could diverse a lot, which also increases
the barrier of universal data abstraction.
D. Service Management
In terms of service management at the edge of the network, we argue that the following four fundamental features
should be supported to guarantee a reliable system, including
differentiation, extensibility, isolation, and reliability.
Differentiation: With the fast growth of IoT deployment,
we expected multiple services will be deployed at the edge
of the network, such as Smart Home. These services will
have different priorities. For example, critical services such as
things diagnosis and failure alarm should be processed earlier
than ordinary service. Health related service, for example, fall
detection or heart failure detection should also have a higher
priority compared with other service such as entertainment.
Extensibility: Extensibility could be a huge challenge at the
edge of the network, unlike a mobile system, the things in the
IoT could be very dynamic. When the owner purchases a new
thing, can it be easily added to the current service without any
problem? Or when one thing is replaced due to wearing out,
can the previous service adopt a new node easily? These problems should be solved with a flexible and extensible design of
service management layer in the edgeOS.
Isolation: Isolation would be another issue at the edge of
the network. In mobile OS, if an application fails or crashes,
the whole system will usually crash and reboot. Or in a distributed system the shared resource could be managed with
different synchronization mechanisms such as a lock or token
ring. However, in a smart edgeOS, this issue might be more
complicated. There could be several applications that share
the same data resource, for example, the control of light. If
one application failed or was not responding, a user should
still be able to control their lights, without crashing the whole
edgeOS. Or when a user removes the only application that
controls lights from the system, the lights should still be alive
rather than experiencing a lost connection to the edgeOS.
This challenge could be potentially solved by introducing a
deployment/undeployment framework. If the conflict could be
detected by the OS before an application is installed, then
a user can be warned and avoid the potential access issue.
Another side of the isolation challenge is how to isolate a
user’s private data from third party applications. For example, your activity tracking application should not be able to
access your electricity usage data. To solve this challenge, a
well-designed control access mechanism should be added to
the service management layer in the edgeOS.
Reliability: Last but not least, reliability is also a key challenge at the edge of the network. We identify the challenges
in reliability from the different views of service, system, and
data here.
1) From the service point of view, it is sometimes very hard
to identify the reason for a service failure accurately at
field. For example, if an air conditioner is not working, a
potential reason could be that a power cord is cut, compressor failure, or even a temperature controller has run
out of battery. A sensor node could have lost connection very easily to the system due to battery outage, bad
connection condition, component wear out, etc. At the
edge of the network, it is not enough to just maintain a
current service when some nodes lose connection, but to
provide the action after node failure makes more sense
to the user. For example, it would be very nice if the
edgeOS could inform the user which component in the
service is not responding, or even alert the user ahead
if some parts in the system have a high risk of failure.
Potential solutions for this challenge could be adapted
from a wireless sensor network, or industrial network
such as PROFINET [29].
2) From the system point of view, it is very important for
the edgeOS to maintain the network topology of the
whole system, and each component in the system is
able to send status/diagnosis information to the edgeOS.
With this feature, services such as failure detection, thing
replacement, and data quality detection could be easily
deployed at the system level.
3) From the data point of view, reliability challenge rise
mostly from the data sensing and communication part.
As previously researched and discussed, things at the
edge of the network could fail due to various reasons and
they could also report low fidelity data under unreliable
condition such as low battery level [30]. Also various
new communication protocols for IoT data collection
are also proposed. These protocols serves well for the
support of huge number of sensor nodes and the highly
dynamic network condition [31]. However, the connection reliability is not as good as BlueTooth or WiFi.
If both sensing data and communication are not reliable, how the system can still provide reliable service by
leveraging multiple reference data source and historical
data record is still an open challenge.
E. Privacy and Security
At the edge of the network, usage privacy and data security protection are the most important services that should be
provided. If a home is deployed with IoT, a lot of privacy
information can be learned from the sensed usage data. For
example, with the reading of the electricity or water usage,
one can easily speculate if the house is vacant or not. In this
case, how to support service without harming privacy is a
challenge. Some of the private information could be removed
from data before processing such as masking all the faces in
the video. We think that keeping the computing at the edge
of data resource, which means in the home, could be a decent
method to protect privacy and data security. To protect the data
security and usage privacy at the edge of the network, several
challenges remain open.
First is the awareness of privacy and security to the community. We take WiFi networks security as an example. Among
the 439 million households who use wireless connections,
49% of WiFi networks are unsecured, and 80% of households still have their routers set on default passwords. For
public WiFi hotspots, 89% of them are unsecured [32]. All
the stake holders including service provider, system and application developer and end user need to aware that the users’
privacy would be harmed without notice at the edge of the network. For example, ip camera, health monitor, or even some
WiFi enabled toys could easily be connected by others if not
protected properly.
Second is the ownership of the data collected from things at
edge. Just as what happened with mobile applications, the data
of end user collected by things will be stored and analyzed at
the service provider side. However, leave the data at the edge
where it is collected and let the user fully own the data will be
a better solution for privacy protection. Similar to the health
record data, end user data collected at the edge of the network
should be stored at the edge and the user should be able to
control if the data should be used by service providers. During
the process of authorization, highly private data could also be
removed by the things to further protect user privacy.
Third is the missing of efficient tools to protect data privacy and security at the edge of the network. Some of the
things are highly resource constrained so the current methods for security protection might not be able to be deployed
on thing because they are resource hungry. Moreover, the
highly dynamic environment at the edge of the network also
makes the network become vulnerable or unprotected. For
privacy protection, some platform such as Open mHealth is
proposed to standardize and store health data [33], but more
tools are still missing to handle diverse data attributes for edge
computing.
F. Optimization Metrics
In edge computing, we have multiple layers with different
computation capability. Workload allocation becomes a big
issue. We need to decide which layer to handle the workload
or how many tasks to assign at each part. There are multiple allocation strategies to complete a workload, for instances,
evenly distribute the workload on each layer or complete as
much as possible on each layer. The extreme cases are fully
operated on endpoint or fully operated on cloud. To choose
an optimal allocation strategy, we discuss several optimization
metrics in this section, including latency, bandwidth, energy
and cost.
Latency: Latency is one of the most important metrics to
evaluate the performance, especially in interaction applications/services [34], [35]. Servers in cloud computing provide
high computation capability. They can handle complex workloads in a relatively short time, such as image processing,
voice recognition and so on. However, latency is not only
determined by computation time. Long WAN delays can
dramatically influence the real-time/interaction intensive applications’ behavior [36]. To reduce the latency, the workload
should better be finished in the nearest layer which has enough
computation capability to the things at the edge of the network.
For example, in the smart city case, we can leverage phones
to process their local photos first then send a potential missing
child’s info back to the cloud instead of uploading all photos.
Due to the large amount of photos and their size, it will be
much faster to preprocess at the edge. However, the nearest
physical layer may not always be a good option. We need to
consider the resource usage information to avoid unnecessary
waiting time so that a logical optimal layer can be found. If a
user is playing games, since the phone’s computation resource
is already occupied, it will be better to upload a photo to the
nearest gateway or micro-center.
Bandwidth: From latency’s point of view, high bandwidth
can reduce transmission time, especially for large data (e.g.,
video, etc.) [37], [38]. For short distance transmission, we can
establish high bandwidth wireless access to send data to the
edge. On one hand, if the workload can be handled at the
edge, the latency can be greatly improved compared to work
on the cloud. The bandwidth between the edge and the cloud
is also saved. For example, in the smart home case, almost all
the data can be handled in the home gateway through Wi-Fi or
other high speed transmission methods. In addition, the transmission reliability is also enhanced as the transmission path is
short. On the other hand, although the transmission distance
cannot be reduced since the edge cannot satisfy the computation demand, at least the data is preprocessed at the edge and
the upload data size will be significantly reduced. In the smart
city case, it is better to preprocess photos before upload, so
the data size can be greatly reduced. It saves the users’ bandwidth, especially if they are using a carriers’ data plan. From
a global perspective, the bandwidth is saved in both situations, and it can be used by other edges to upload/download
data. Hence, we need to evaluate if a high bandwidth connection is needed and which speed is suitable for an edge.
Besides, to correctly determine the workload allocation in each
layer, we need to consider the computation capability and
bandwidth usage information in layers to avoid competition
and delay.
Energy: Battery is the most precious resource for things
at the edge of the network. For the endpoint layer, offloading workload to the edge can be treated as an energy free
method [22], [39]. So for a given workload, is it energy efficient to offload the whole workload (or part of it) to the edge
rather than compute locally? The key is the tradeoff between
the computation energy consumption and transmission energy
consumption. Generally speaking, we first need to consider the
power characteristics of the workload. Is it computation intensive? How much resource will it use to run locally? Besides the
network signal strength [40], the data size and available bandwidth will also influence the transmission energy overhead
[28]. We prefer to use edge computing only if the transmission overhead is smaller than computing locally. However, if
we care about the whole edge computing process rather than
only focus on endpoints, total energy consumption should be
the accumulation of each used layer’s energy cost. Similar to
the endpoint layer, each layer’s energy consumption can be
estimated as local computation cost plus transmission cost. In
this case, the optimal workload allocation strategy may change.
For example, the local data center layer is busy, so the workload is continuously uploaded to the upper layer. Comparing
with computing on endpoints, the multihop transmission may
dramatically increase the overhead which causes more energy
consumption.
Cost: From the service providers’ perspective, e.g.,
YouTube, Amazon, etc., edge computing provides them less
latency and energy consumption, potential increased throughput and improved user experience. As a result, they can earn
more money for handling the same unit of workload. For
example, based on most residents’ interest, we can put a popular video on the building layer edge. The city layer edge can
free from this task and handle more complex work. The total
throughput can be increased. The investment of the service
providers is the cost to build and maintain the things in each
layer. To fully utilize the local data in each layer, providers
can charge users based on the data location. New cost models
need to be developed to guarantee the profit of the service
provider as well as acceptability of users.
Workload allocation is not an easy task. The metrics are
closely related to each other. For example, due to the energy
constraints, a workload needs to be complete on the city data
center layer. Comparing with the building server layer, the
energy limitation inevitably affects the latency. Metrics should
be given priority (or weight) for different workloads so that
a reasonable allocation strategy can be selected. Besides, the
cost analysis needs to be done in runtime. The interference and
resource usage of concurrent workloads should be considered
as well."